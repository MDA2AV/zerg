{"/zerg/about/":{"data":{"":"","acknowledgements#Acknowledgements":"zerg builds on the following:\nliburing – userspace library for io_uring io_uring – Linux kernel async I/O interface by Jens Axboe Dmitry Vyukov’s bounded MPMC queue – basis for the MPSC queue implementations","author#Author":"Diogo Martins","license#License":"MIT License","links#Links":"GitHub Repository NuGet Package","version#Version":"Current release: v0.3.12\nTarget frameworks: .NET 9.0, .NET 10.0","zerg#zerg":"zerg (uR(ing)(S)ocket) is an experimental, high-performance TCP server framework for C# built directly on Linux io_uring.\nIt provides low-level control over sockets, buffers, queues, and scheduling with a focus on zero-allocation hot paths and maximum throughput."},"title":"About"},"/zerg/docs/":{"data":{"":"Welcome to the zerg documentation. zerg is a high-performance TCP server framework for C# built directly on Linux io_uring.","key-features#Key Features":"io_uring multishot accept – single submission produces a CQE for every new connection io_uring multishot recv with buffer selection – kernel picks from a pre-registered buffer pool per packet Zero-allocation hot path – unmanaged memory, ValueTask, lock-free queues Reactor-per-thread model – each reactor independently manages its connections Flexible read API – from raw RingItem pointers to ReadOnlySequence IBufferWriter write path – direct span writes with async flush to kernel Stream adapter – ConnectionStream for BCL/pipeline compatibility","what-is-zerg#What is zerg?":"zerg is a low-level networking library that gives you direct control over sockets, buffers, queues, and scheduling through the Linux io_uring async I/O interface. It implements the reactor pattern with one acceptor thread and N reactor threads, each owning their own io_uring instance for zero-contention I/O processing.","where-to-start#Where to Start":"InstallationNuGet package, native dependencies, and build from source Quick StartMinimal server with a connection handler in under 30 lines ArchitectureUnderstand the acceptor + reactor threading model"},"title":"Documentation"},"/zerg/docs/api-reference/":{"data":{"":"Complete reference for all public APIs in zerg.\nEngineEngine, Listen, AcceptAsync, Stop Connection: ReadReadAsync, ReadResult, RingItem, ReturnRing Connection: WriteWrite, IBufferWriter, FlushAsync ConnectionStreamBCL Stream adapter for compatibility ConfigurationFull config reference tables"},"title":"API Reference"},"/zerg/docs/api-reference/configuration/":{"data":{"":"Complete reference for all configuration types in zerg.","acceptorconfig#AcceptorConfig":"Acceptor thread configuration. Sealed record with default values.\npublic sealed record AcceptorConfig( uint RingFlags = 0, int SqCpuThread = -1, uint SqThreadIdleMs = 100, uint RingEntries = 8 * 1024, uint BatchSqes = 4096, long CqTimeout = 100_000_000, IPVersion IPVersion = IPVersion.IPv6DualStack ); Property Type Default Description RingFlags uint 0 io_uring setup flags for the acceptor ring. SqCpuThread int -1 CPU core for SQPOLL thread. SqThreadIdleMs uint 100 SQPOLL idle timeout (ms). RingEntries uint 8192 SQ/CQ size. Bounds accept CQE burst capacity. BatchSqes uint 4096 Max accept completions per loop iteration. CqTimeout long 100000000 Wait timeout in ns (100 ms). Higher than reactor since accepts are bursty. IPVersion IPVersion IPv6DualStack IP stack for the listening socket.","configuration-examples#Configuration Examples":"","engineoptions#EngineOptions":"Top-level configuration passed to new Engine(options).\npublic class EngineOptions { public int ReactorCount { get; init; } = 1; public string Ip { get; init; } = \"0.0.0.0\"; public ushort Port { get; init; } = 8080; public int Backlog { get; init; } = 65535; public AcceptorConfig AcceptorConfig { get; init; } = new(); public ReactorConfig[] ReactorConfigs { get; set; } = null!; } Property Type Default Description ReactorCount int 1 Number of reactor event-loop threads. Each gets its own io_uring. Ip string \"0.0.0.0\" Bind address. \"0.0.0.0\" for all IPv4, \"::\" for all IPv6. Port ushort 8080 TCP listen port. Backlog int 65535 Kernel listen backlog (pending connection queue size). AcceptorConfig AcceptorConfig new() Acceptor ring configuration. ReactorConfigs ReactorConfig[] null Per-reactor configs. Auto-filled with defaults if null.","high-throughput-server#High-Throughput Server":"var engine = new Engine(new EngineOptions { Port = 8080, ReactorCount = Environment.ProcessorCount, ReactorConfigs = Enumerable.Range(0, Environment.ProcessorCount) .Select(_ =\u003e new ReactorConfig( RingEntries: 16384, RecvBufferSize: 64 * 1024, BufferRingEntries: 32 * 1024, BatchCqes: 8192, MaxConnectionsPerReactor: 16384, CqTimeout: 500_000 // 0.5 ms )).ToArray() });","io_uring-setup-flags#io_uring Setup Flags":"Available constants from the ABI class for use in RingFlags:\nFlag Value Description IORING_SETUP_IOPOLL 1 \u003c\u003c 0 Polling-based I/O (not recommended for sockets). IORING_SETUP_SQPOLL 1 \u003c\u003c 1 Kernel thread polls SQ. Eliminates submit syscall. IORING_SETUP_SQ_AFF 1 \u003c\u003c 2 Pin SQPOLL thread to SqCpuThread. IORING_SETUP_CQSIZE 1 \u003c\u003c 3 Custom CQ size (separate from SQ). IORING_SETUP_CLAMP 1 \u003c\u003c 4 Clamp queue sizes to kernel limits. IORING_SETUP_SINGLE_ISSUER 1 \u003c\u003c 12 Single thread submits. Reduces kernel locking. IORING_SETUP_DEFER_TASKRUN 1 \u003c\u003c 13 Defer task_work. Reduces latency spikes. IORING_SETUP_NO_MMAP 1 \u003c\u003c 14 Don’t mmap rings (use registered buffers). IORING_SETUP_REGISTERED_FD_ONLY 1 \u003c\u003c 15 Only allow registered file descriptors.","ipversion#IPVersion":"public enum IPVersion { IPv4Only, IPv6DualStack } Value Socket Description IPv4Only AF_INET IPv4 only. IPv6DualStack AF_INET6 (V6ONLY=0) Accepts both IPv4 and IPv6. IPv4 clients appear as ::ffff:a.b.c.d.","low-latency-server-with-sqpoll#Low-Latency Server with SQPOLL":"var engine = new Engine(new EngineOptions { Port = 8080, ReactorCount = 4, ReactorConfigs = Enumerable.Range(0, 4) .Select(i =\u003e new ReactorConfig( RingFlags: ABI.IORING_SETUP_SQPOLL | ABI.IORING_SETUP_SQ_AFF | ABI.IORING_SETUP_SINGLE_ISSUER, SqCpuThread: i + 4, // pin SQPOLL threads to CPUs 4-7 SqThreadIdleMs: 200, CqTimeout: 100_000 // 0.1 ms )).ToArray() });","minimal-memory-footprint#Minimal Memory Footprint":"var engine = new Engine(new EngineOptions { Port = 8080, ReactorCount = 1, ReactorConfigs = [new ReactorConfig( RecvBufferSize: 4096, BufferRingEntries: 1024, MaxConnectionsPerReactor: 256, RingEntries: 512 )] });","reactorconfig#ReactorConfig":"Per-reactor configuration. Sealed record with default values.\npublic sealed record ReactorConfig( uint RingFlags = IORING_SETUP_SINGLE_ISSUER | IORING_SETUP_DEFER_TASKRUN, int SqCpuThread = -1, uint SqThreadIdleMs = 100, uint RingEntries = 8 * 1024, int RecvBufferSize = 32 * 1024, int BufferRingEntries = 16 * 1024, int BatchCqes = 4096, int MaxConnectionsPerReactor = 8 * 1024, long CqTimeout = 1_000_000 ); Property Type Default Description RingFlags uint SINGLE_ISSUER | DEFER_TASKRUN io_uring setup flags. Controls kernel behavior. SqCpuThread int -1 CPU core for SQPOLL thread. -1 = kernel decides. Only with SQPOLL. SqThreadIdleMs uint 100 SQPOLL idle timeout (ms) before kernel thread sleeps. RingEntries uint 8192 SQ/CQ ring size. Max in-flight I/O operations. RecvBufferSize int 32768 Bytes per receive buffer. Larger = fewer buffers for big payloads. BufferRingEntries int 16384 Number of recv buffers in buffer ring. Must be power of 2. BatchCqes int 4096 Max CQEs processed per event loop iteration. MaxConnectionsPerReactor int 8192 Connection limit per reactor. Should be \u003c= RingEntries. CqTimeout long 1000000 Wait timeout in nanoseconds (1 ms). Lower = lower latency, more CPU."},"title":"Configuration Reference"},"/zerg/docs/api-reference/connection-read/":{"data":{"":"The read API provides async waiting for inbound data, snapshot-based batch draining, and buffer management.","buffer-return#Buffer Return":"","core-read-cycle#Core Read Cycle":"Every read follows this pattern:\n// 1. Wait for data ReadResult result = await connection.ReadAsync(); // 2. Check for close if (result.IsClosed) return; // 3. Drain and process buffers // 4. Return buffers to kernel // 5. Reset for next read connection.ResetRead();","enqueueringitem-internal#EnqueueRingItem (Internal)":"public void EnqueueRingItem(byte* ptr, int length, ushort bufferId) Called by the reactor when a recv CQE completes. Enqueues a RingItem into the connection’s SPSC ring and wakes the handler if armed.\nIf the ring is full (1024 items), the connection is closed If the handler is armed (_armed == 1), it’s woken immediately If no handler is armed, _pending is set for the next ReadAsync() fast-path","getallsnapshotrings#GetAllSnapshotRings":"public RingItem[] GetAllSnapshotRings(ReadResult readResult) Returns raw RingItem values from the snapshot. Each item has Ptr, Length, and BufferId.\nRingItem[] items = connection.GetAllSnapshotRings(result); foreach (var item in items) { ReadOnlySpan\u003cbyte\u003e data = item.AsSpan(); // process... connection.ReturnRing(item.BufferId); }","getallsnapshotringsasunmanagedmemory#GetAllSnapshotRingsAsUnmanagedMemory":"public UnmanagedMemoryManager[] GetAllSnapshotRingsAsUnmanagedMemory(ReadResult readResult) Returns an array of UnmanagedMemoryManager instances, one per received buffer in the snapshot. Each wraps a native pointer with a managed Memory view.\nUse with ToReadOnlySequence() for parsing:\nvar rings = connection.GetAllSnapshotRingsAsUnmanagedMemory(result); ReadOnlySequence\u003cbyte\u003e sequence = rings.ToReadOnlySequence(); // parse sequence... rings.ReturnRingBuffers(connection.Reactor);","getring#GetRing":"public RingItem GetRing() Dequeue one item unconditionally. Assumes the ring is not empty. Use only when you know items are available.","high-level-drain-apis#High-Level Drain APIs":"These methods dequeue all items in the current snapshot batch. Call one of these after ReadAsync() returns successfully.","low-level-drain-apis#Low-Level Drain APIs":"For fine-grained control over individual ring items.","markclosed-internal#MarkClosed (Internal)":"Sets _closed = 1 and wakes any armed handler so it receives ReadResult.Closed().","properties#Properties":"","reactor-side-producer-api#Reactor-Side Producer API":"These methods are called by the reactor thread, not by user code:","readasync#ReadAsync":"public ValueTask\u003cReadResult\u003e ReadAsync() Waits for at least one received buffer to be available, or for the connection to close.\nReturns: A ReadResult containing a tail snapshot and close status.\nFast paths (returns synchronously):\nConnection already closed → ReadResult.Closed() Data already pending in the receive ring → immediate result with snapshot _pending flag set from previous produce → immediate result Slow path:\nParks the calling task until the reactor enqueues data or marks the connection closed Rules:\nOnly one outstanding ReadAsync per connection at a time (single waiter) After processing the batch, call ResetRead() before the next ReadAsync()","readresult#ReadResult":"public readonly struct ReadResult { public long TailSnapshot { get; } public bool IsClosed { get; } } Property Description TailSnapshot Logical position in the receive ring at the time of read. Defines the batch boundary – you can drain items up to this position. IsClosed true if the connection was closed (EOF, error, or reuse).","resetread#ResetRead":"public void ResetRead() Prepares the connection for the next read cycle. Must be called after draining a batch and before the next ReadAsync().\nInternally:\nResets the ManualResetValueTaskSourceCore (it’s single-use) Checks if new data arrived during processing and sets _pending = 1 if so","returnring#ReturnRing":"public void ReturnRing(ushort bufferId) Returns a consumed buffer to the reactor’s buffer ring. The reactor will add it back to the kernel buffer ring on its next loop iteration.\nMust be called for every buffer obtained via TryGetRing, GetRing, GetAllSnapshotRings, etc.","returnringbuffers-extension-method#ReturnRingBuffers (Extension Method)":"public static void ReturnRingBuffers(this UnmanagedMemoryManager[] managers, Engine.Engine.Reactor reactor) Batch return of buffer IDs from an array of UnmanagedMemoryManager:\nvar rings = connection.GetAllSnapshotRingsAsUnmanagedMemory(result); // process... rings.ReturnRingBuffers(connection.Reactor);","ringitem#RingItem":"public readonly unsafe struct RingItem(byte* ptr, int length, ushort bufferId) { public byte* Ptr { get; } public int Length { get; } public ushort BufferId { get; } public ReadOnlySpan\u003cbyte\u003e AsSpan(); public UnmanagedMemoryManager AsUnmanagedMemoryManager(); } A lightweight struct wrapping a received data buffer from the kernel. The pointer is valid until ReturnRing(BufferId) is called.","snapshotringcount#SnapshotRingCount":"public int SnapshotRingCount { get; } Number of items in the current snapshot batch. Set when ReadAsync() captures the tail.","totalringcount#TotalRingCount":"public long TotalRingCount { get; } Current number of items in the receive ring (approximate, for diagnostics).","trydynamicallygetallsnapshotrings#TryDynamicallyGetAllSnapshotRings":"public bool TryDynamicallyGetAllSnapshotRings( ReadResult readResult, out List\u003cRingItem\u003e rings) Dequeues all segments as raw RingItem values. Returns false if no data.","trydynamicallygetallsnapshotringsasreadonlysequence#TryDynamicallyGetAllSnapshotRingsAsReadOnlySequence":"public bool TryDynamicallyGetAllSnapshotRingsAsReadOnlySequence( ReadResult readResult, out List\u003cUnmanagedMemoryManager\u003e rings, out ReadOnlySequence\u003cbyte\u003e sequence) Builds a zero-copy ReadOnlySequence over all segments. Returns false if no data is available.","trydynamicallygetallsnapshotringsasunmanagedmemory#TryDynamicallyGetAllSnapshotRingsAsUnmanagedMemory":"public bool TryDynamicallyGetAllSnapshotRingsAsUnmanagedMemory( ReadResult readResult, out List\u003cUnmanagedMemoryManager\u003e rings) Dequeues all segments as a list of UnmanagedMemoryManager. Returns false if no data.","trygetring#TryGetRing":"public bool TryGetRing(long tailSnapshot, out RingItem item) Dequeue one item from the receive ring, bounded by the snapshot. Returns false when the batch is exhausted.\nwhile (connection.TryGetRing(result.TailSnapshot, out RingItem ring)) { ReadOnlySpan\u003cbyte\u003e data = ring.AsSpan(); // process one buffer... connection.ReturnRing(ring.BufferId); }"},"title":"Connection: Read"},"/zerg/docs/api-reference/connection-stream/":{"data":{"":"ConnectionStream is a thin Stream adapter that bridges zerg’s native connection API to BCL pipeline APIs. It provides zero-copy writes and single-copy reads for compatibility with code that expects a System.IO.Stream.","class-definition#Class Definition":"public sealed class ConnectionStream : Stream","constructor#Constructor":"public ConnectionStream(Connection inner) Wraps an existing Connection in a Stream interface. The underlying connection must already be registered with a reactor.","design-notes#Design Notes":"No internal buffering: Reads pull directly from reactor rings; writes go directly to the slab No synchronization: The reactor provides exclusivity guarantees Single copy on read: Data is copied from kernel-provided buffers into the caller’s buffer Zero-copy on write: Data is copied into the slab (same as native Write())","disposal#Disposal":"protected override void Dispose(bool disposing) Idempotent disposal using an interlocked guard. Disposes the underlying connection. Safe to call multiple times.","flush#Flush":"","flush-1#Flush()":"public override void Flush() Throws NotSupportedException. Synchronous flush is not supported. Use FlushAsync().","flushasynccancellationtoken#FlushAsync(CancellationToken)":"public override Task FlushAsync(CancellationToken token) Flushes all previously written data. Delegates to connection.FlushAsync().AsTask(). The reactor controls the actual send – the returned Task completes when all staged bytes have been transmitted.","read-operations#Read Operations":"","readasyncbyte-int-int-cancellationtoken#ReadAsync(byte[], int, int, CancellationToken)":"public override Task\u003cint\u003e ReadAsync(byte[] buffer, int offset, int count, CancellationToken cancellationToken) Legacy array-based read. Delegates to the Memory overload.","readasyncmemorybyte-cancellationtoken#ReadAsync(Memory\u0026lt;byte\u0026gt;, CancellationToken)":"public override ValueTask\u003cint\u003e ReadAsync(Memory\u003cbyte\u003e destination, CancellationToken cancellationToken = default) Reads from the reactor receive rings and copies into destination.\nSteps:\nAwaits the next receive snapshot from the reactor (connection.ReadAsync()) Returns 0 (EOF) if the connection is closed Gathers all segments belonging to the snapshot Copies once into the caller’s buffer via CopyFromRings() Returns each ring buffer to the reactor pool Calls ResetRead() to prepare for the next cycle Returns the number of bytes copied","readbyte-int-int#Read(byte[], int, int)":"public override int Read(byte[] buffer, int offset, int count) Throws NotSupportedException. Synchronous reads are not supported on an async reactor-driven connection.","supported-operations#Supported Operations":"Property Value Description CanRead true Reading is supported via reactor receive rings CanWrite true Writing appends to the connection’s unmanaged slab CanSeek false Network streams are not seekable","unsupported-operations#Unsupported Operations":"Method Behavior Read(byte[], int, int) Throws NotSupportedException Flush() Throws NotSupportedException Seek(long, SeekOrigin) Throws NotSupportedException SetLength(long) Throws NotSupportedException Length (property) Throws NotSupportedException Position (property) Throws NotSupportedException","usage-example#Usage Example":"static async Task HandleWithStreamAsync(Connection connection) { await using var stream = new ConnectionStream(connection); var buffer = new byte[4096]; int bytesRead; while ((bytesRead = await stream.ReadAsync(buffer)) \u003e 0) { // Process data in buffer[..bytesRead] // Echo back await stream.WriteAsync(buffer.AsMemory(0, bytesRead)); await stream.FlushAsync(); } }","when-to-use-connectionstream#When to Use ConnectionStream":"Use ConnectionStream when you need to integrate with APIs that require Stream:\nSystem.Text.Json serialization/deserialization StreamReader/StreamWriter for text protocols Third-party libraries that accept Stream PipeReader/PipeWriter adapters For maximum performance, prefer the native Connection API directly (ReadAsync/Write/FlushAsync). ConnectionStream adds one copy on reads (from kernel buffers into your destination buffer) and wraps ValueTask as Task for FlushAsync.","write-operations#Write Operations":"","writeasyncreadonlymemorybyte-cancellationtoken#WriteAsync(ReadOnlyMemory\u0026lt;byte\u0026gt;, CancellationToken)":"public override ValueTask WriteAsync(ReadOnlyMemory\u003cbyte\u003e buffer, CancellationToken cancellationToken = default) Fast async write with no allocation and no implicit flush. Returns a completed ValueTask if the buffer is empty. Otherwise copies the data into the write slab synchronously and returns completed.\nNote: This does not flush. Call FlushAsync() to send data.","writebyte-int-int#Write(byte[], int, int)":"public override void Write(byte[] buffer, int offset, int count) Synchronous write. Validates parameters and copies the buffer slice into the connection’s unmanaged write slab."},"title":"ConnectionStream"},"/zerg/docs/api-reference/connection-write/":{"data":{"":"The write API provides a staged write buffer and async flush to the kernel. Data is written into an unmanaged slab, then flushed via io_uring send operations.","advance#Advance":"public void Advance(int count) Advances the write tail by count bytes. Call after writing into the span/memory returned by GetSpan/GetMemory.\nThrows:\nInvalidOperationException if a flush is in progress","flush-completion#Flush Completion":"The reactor handles the flush-to-kernel process:\nDrain flush queue: flushQ.TryDequeue(out clientFd) Prepare send: shim_prep_send(sqe, fd, writeBuffer + writeHead, writeInFlight - writeHead, 0) Submit and process CQE: cqe-\u003eres = bytes sent Advance WriteHead by bytes sent If WriteHead \u003c WriteInFlight: resubmit for remaining bytes If complete: call CompleteFlush() on the connection Handler Reactor │ │ │── Write(data) ──────▶│ │── Write(data) ──────▶│ │── FlushAsync() ─────▶│ enqueue fd to flushQ │ (awaiting) │ │ │── drain flushQ │ │── prep_send(fd, buf, len) │ │── submit │ │ ... │ │── CQE: send complete │◀── CompleteFlush() ──│ resume ValueTask │ │ │── Write(next) ──────▶│ slab reset, ready for next cycle","flushasync#FlushAsync":"public ValueTask FlushAsync() Arms a flush and returns a ValueTask that completes when the reactor has sent all staged bytes.\nBehavior:\nCaptures WriteTail as the flush target (WriteInFlight) Sets _flushInProgress = 1 (blocks further writes) Enqueues the connection’s fd to the reactor’s flush queue The reactor issues a send SQE with the staged bytes If partial send, the reactor resubmits for the remaining bytes When fully sent, the reactor calls CompleteFlush() which: Clears _flushInProgress Resets WriteHead and WriteTail to 0 Completes the ValueTask Fast path: Returns default(ValueTask) (completed) if nothing was written (WriteTail == 0).\nThrows:\nInvalidOperationException if a flush is already in progress InvalidOperationException if a flush is already armed","getmemory#GetMemory":"public Memory\u003cbyte\u003e GetMemory(int sizeHint = 0) Returns a writable Memory over the remaining slab space. Useful for APIs that require Memory.","getspan#GetSpan":"public Span\u003cbyte\u003e GetSpan(int sizeHint = 0) Returns a writable Span over the remaining slab space. Write directly into this span, then call Advance().\nSpan\u003cbyte\u003e span = connection.GetSpan(256); int written = Encoding.UTF8.GetBytes(\"Hello\", span); connection.Advance(written);","ibufferwriterbyte#IBufferWriter\u0026lt;byte\u0026gt;":"The Connection class implements IBufferWriter, allowing direct writes into the slab without copying:","slab-layout#Slab Layout":"┌──────────────────────────────────────────┐ │ Unmanaged Write Slab │ │ (64-byte aligned, default 16 KB) │ │ │ │ ┌─────────┬──────────────┬──────────┐ │ │ │ Sent │ Staged data │ Free │ │ │ │ (Head) │ │ space │ │ │ └─────────┴──────────────┴──────────┘ │ │ ^WriteHead ^WriteTail │ └──────────────────────────────────────────┘ Field Description WriteBuffer Pointer to the start of the 64-byte aligned slab WriteHead Start of valid staged data (currently always 0) WriteTail End of written data – advanced by Write() and Advance() WriteInFlight Snapshot of WriteTail captured by FlushAsync() SendInflight Reactor-owned flag: 1 if a send SQE is outstanding","thread-safety#Thread Safety":"Write(), GetSpan(), Advance() must be called from a single thread (the handler) FlushAsync() enqueues to an MPSC queue (safe from any thread) CompleteFlush() is called by the reactor thread, which may resume the handler inline The _flushInProgress flag prevents writes during an active flush","write-buffer-internals#Write Buffer Internals":"","write-methods#Write Methods":"","write-slab-size#Write Slab Size":"The default write slab is 16 KB per connection, allocated as 64-byte aligned unmanaged memory:\nvar connection = new Connection(writeSlabSize: 1024 * 16); If your responses are larger than 16 KB, increase this value. The entire response must fit in the slab before flushing.","writebyte-int-unsafe#Write(byte*, int) (Unsafe)":"public void Write(byte* ptr, int length) Copies from unmanaged memory into the write slab. Uses Buffer.MemoryCopy for native-to-native copy.","writeflush-cycle#Write/Flush Cycle":"// 1. Stage data into the write buffer connection.Write(\"HTTP/1.1 200 OK\\r\\n\"u8); connection.Write(\"Content-Length: 13\\r\\n\\r\\n\"u8); connection.Write(\"Hello, World!\"u8); // 2. Flush staged data to kernel await connection.FlushAsync(); // Write buffer is automatically reset after flush completes","writereadonlymemorybyte#Write(ReadOnlyMemory\u0026lt;byte\u0026gt;)":"public void Write(ReadOnlyMemory\u003cbyte\u003e source) Same behavior as the span overload, using source.Span.","writereadonlyspanbyte#Write(ReadOnlySpan\u0026lt;byte\u0026gt;)":"public void Write(ReadOnlySpan\u003cbyte\u003e source) Copies the span into the unmanaged write slab at the current tail position.\nThrows:\nInvalidOperationException if a flush is in progress InvalidOperationException if the slab doesn’t have enough space"},"title":"Connection: Write"},"/zerg/docs/api-reference/engine/":{"data":{"":"The Engine class is the entry point for zerg. It creates the acceptor and reactor threads, manages the listening socket, and provides the AcceptAsync API for receiving new connections.","acceptasynccancellationtoken#\u003ccode\u003eAcceptAsync(CancellationToken)\u003c/code\u003e":"public async ValueTask\u003cConnection?\u003e AcceptAsync(CancellationToken cancellationToken = default) Waits for the next accepted connection and returns the fully registered Connection object. The connection is already assigned to a reactor and has multishot recv armed by the time this method returns.\nReturns null if the server is shutting down or the cancellation token is triggered.\nUsage:\nwhile (engine.ServerRunning) { var connection = await engine.AcceptAsync(cts.Token); if (connection is null) continue; _ = HandleConnectionAsync(connection); } Important: AcceptAsync blocks until a connection is available. The returned connection is ready for immediate ReadAsync().","acceptor#\u003ccode\u003eAcceptor\u003c/code\u003e":"The acceptor manages the listening socket and multishot accept. Created internally by Listen().","class-definition#Class Definition":"namespace zerg.Engine; public partial class Engine","connections#\u003ccode\u003eConnections\u003c/code\u003e":"public Dictionary\u003cint, Connection\u003e[] Connections { get; set; } Per-reactor connection dictionaries mapping file descriptors to Connection objects.","constructors#Constructors":"","engine#\u003ccode\u003eEngine()\u003c/code\u003e":"Creates an engine with default EngineOptions.\nvar engine = new Engine(); // Defaults: Port=8080, ReactorCount=1, Ip=\"0.0.0.0\"","engineengineoptions-options#\u003ccode\u003eEngine(EngineOptions options)\u003c/code\u003e":"Creates an engine with custom configuration.\nvar engine = new Engine(new EngineOptions { Port = 8080, ReactorCount = Environment.ProcessorCount, Backlog = 65535 }); If options.ReactorConfigs is null, it is auto-initialized with default ReactorConfig for each reactor.","example-full-server-lifecycle#Example: Full Server Lifecycle":"var engine = new Engine(new EngineOptions { Port = 8080, ReactorCount = 4 }); engine.Listen(); Console.WriteLine(\"Server started on :8080 with 4 reactors\"); var cts = new CancellationTokenSource(); // Shutdown hook Console.CancelKeyPress += (_, e) =\u003e { e.Cancel = true; engine.Stop(); cts.Cancel(); }; try { while (engine.ServerRunning) { var conn = await engine.AcceptAsync(cts.Token); if (conn is null) continue; _ = HandleConnectionAsync(conn); } } catch (OperationCanceledException) { } Console.WriteLine(\"Server stopped.\");","listen#\u003ccode\u003eListen()\u003c/code\u003e":"public void Listen() Starts the engine:\nCreates the acceptor with the configured AcceptorConfig Creates N reactor instances (one per ReactorCount) Initializes per-reactor ConcurrentQueue for fd distribution Initializes per-reactor connection dictionaries Starts reactor threads (each enters its event loop) Starts the acceptor thread (arms multishot accept) Prints diagnostics to console After Listen() returns, the server is accepting connections. Call AcceptAsync() to receive them.","methods#Methods":"","nested-types#Nested Types":"","options#\u003ccode\u003eOptions\u003c/code\u003e":"public EngineOptions Options { get; } The engine configuration. Read-only after construction.","properties#Properties":"","reactor#\u003ccode\u003eReactor\u003c/code\u003e":"Each reactor manages an io_uring instance, buffer ring, and a set of connections. See Threading Model for how reactors interact with handlers.\nKey reactor methods (internal, called by Connection):\nMethod Description EnqueueReturnQ(ushort bufferId) Queue a buffer ID for return to the kernel buffer ring EnqueueFlush(int clientFd) Queue a connection for send by the reactor","reactorconnectioncounts#\u003ccode\u003eReactorConnectionCounts\u003c/code\u003e":"static long[] ReactorConnectionCounts Per-reactor connection counters for metrics and diagnostics. Index corresponds to reactor ID.","reactors#\u003ccode\u003eReactors\u003c/code\u003e":"public Reactor[] Reactors { get; set; } Array of reactor instances. Each reactor owns its own io_uring, buffer ring, and connection map. Index corresponds to reactor ID.","serverrunning#\u003ccode\u003eServerRunning\u003c/code\u003e":"public bool ServerRunning { get; private set; } Global flag checked by the acceptor and all reactor threads. When Stop() is called, this becomes false and all event loops exit gracefully.","singleacceptor#\u003ccode\u003eSingleAcceptor\u003c/code\u003e":"public Acceptor SingleAcceptor { get; set; } The acceptor instance responsible for listening and distributing connections.","static-fields#Static Fields":"","stop#\u003ccode\u003eStop()\u003c/code\u003e":"public void Stop() Signals all event loops (acceptor + reactors) to exit. Sets ServerRunning = false. This is a non-blocking signal – threads will stop once they observe the flag change on their next loop iteration.\nAfter calling Stop():\nThe acceptor closes the listening socket and destroys its ring Each reactor closes all active connections, frees buffer rings, and destroys its ring Any pending AcceptAsync() returns null"},"title":"Engine"},"/zerg/docs/architecture/":{"data":{"":"This section explains the core architectural decisions behind zerg: how threads are organized, how io_uring is used, how buffers flow through the system, and how connections are managed.\nReactor PatternAcceptor + N reactor threads, event loop design io_uringLinux async I/O primer and features used by zerg Buffer RingsProvided buffers lifecycle and zero-copy receive Connection LifecycleAccept, use, close, and return to pool Threading ModelThread layout, memory ordering, and synchronization"},"title":"Architecture"},"/zerg/docs/architecture/buffer-rings/":{"data":{"":"Buffer rings (also called “provided buffers”) are a core io_uring feature that zerg uses for zero-copy receive operations. Instead of userspace providing a buffer with each recv call, the kernel picks from a pre-registered pool.","buffer-lifecycle#Buffer Lifecycle":"┌──────────────────────────────────────────┐ │ Buffer Ring │ │ ┌─────┬─────┬─────┬─────┬─────┐ │ │ │ B0 │ B1 │ B2 │ ... │ Bn │ │ │ └──┬──┴──┬──┴──┬──┴─────┴──┬──┘ │ └─────┼─────┼─────┼───────────┼────────────┘ │ │ │ │ ┌───────────────┘ │ │ └──────────────┐ ▼ ▼ ▼ ▼ ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐ │ Kernel │ │ Kernel │ │ User │ │ User │ │ (free) │ │ (recv) │ │ (proc) │ │ (done) │ └────────┘ └───┬────┘ └───┬────┘ └───┬────┘ │ │ │ │ CQE │ TryGetRing() │ ReturnRing() ▼ ▼ ▼ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ RingItem │ │ Handler │ │ Return Q │ │ enqueued │ │ draining │ │ → kernel │ └──────────┘ └──────────┘ └──────────┘ A buffer transitions through these states:\nState Owner Description Free Kernel Available in the buffer ring for the next recv In-flight Kernel Selected by kernel for an active recv operation Enqueued Reactor Data received, RingItem pushed to connection’s SPSC ring Processing Handler Handler has dequeued the RingItem and is reading the data Returning Return queue Handler called ReturnRing(), buffer ID queued for return Returned Kernel Reactor added buffer back to the ring via buf_ring_add + advance","configuration#Configuration":"Config Property Default Impact BufferRingEntries 16384 Total buffers per reactor. Must be power of two. More buffers = more concurrent in-flight receives. RecvBufferSize 32 KB Size of each buffer. Larger = fewer buffers needed for big payloads, but more memory per buffer. Memory formula: BufferRingEntries * RecvBufferSize per reactor.\nWith defaults: 16384 * 32 KB = 512 MB per reactor.","consumption#Consumption":"The application handler drains RingItems from the connection:\nwhile (connection.TryGetRing(result.TailSnapshot, out RingItem ring)) { ReadOnlySpan\u003cbyte\u003e data = ring.AsSpan(); // zero-copy view // process data... connection.ReturnRing(ring.BufferId); // return to kernel }","how-buffer-rings-work#How Buffer Rings Work":"","important-rules#Important Rules":"Always return buffers. Every buffer obtained via TryGetRing() or GetAllSnapshotRings() must eventually be returned via ReturnRing() or ReturnRingBuffers(). Leaked buffers deplete the pool and eventually stall receives.\nDon’t access buffer data after returning. Once ReturnRing(bufferId) is called, the kernel may reuse that buffer immediately. Any pointer or span referencing the buffer becomes invalid.\nReturn from the handler thread. ReturnRing() enqueues to an MPSC queue that the reactor drains. It’s safe to call from any thread, but typically called from the handler after processing.\nBuffer ring exhaustion. If all buffers are in-flight or held by handlers, new recv operations will fail. The reactor handles this gracefully – multishot recv CQEs may stop arriving until buffers are returned. If the connection’s SPSC ring is full (1024 items), the connection is closed as a safety measure.","receive#Receive":"When multishot recv completes for a connection:\nCQE arrives: cqe-\u003eres = bytes received (or negative errno) cqe-\u003eflags = IORING_CQE_F_BUFFER | (bufferId \u003c\u003c 16) Reactor extracts: bufferId = shim_cqe_buffer_id(cqe) ptr = slab + bufferId * bufSize length = cqe-\u003eres The kernel has already written the received data into the selected buffer. The reactor creates a RingItem(ptr, length, bufferId) and enqueues it to the connection’s receive ring.","return#Return":"When the handler calls ReturnRing(bufferId), the buffer ID is enqueued to the reactor’s return queue (an MPSC queue). On the next loop iteration, the reactor returns all queued buffers to the kernel:\nwhile returnQ.TryDequeue(out bufferId): shim_buf_ring_add(br, slab + bufferId * bufSize, bufSize, bufferId, mask, idx++) shim_buf_ring_advance(br, count)","setup#Setup":"When a reactor starts, it creates a buffer ring and populates it with pre-allocated buffers:\n1. Allocate a contiguous slab of unmanaged memory (64-byte aligned) 2. Register the buffer ring with io_uring: br = shim_setup_buf_ring(ring, entries, bgid, 0, \u0026ret) 3. Divide the slab into fixed-size chunks and add each to the ring: for i in 0..entries: shim_buf_ring_add(br, slab + i * bufSize, bufSize, i, mask, i) 4. Publish all buffers to the kernel: shim_buf_ring_advance(br, entries) Each buffer is identified by a 16-bit bufferId (0 to BufferRingEntries - 1). The slab is a single contiguous allocation so the address of any buffer is slab + bufferId * RecvBufferSize."},"title":"Buffer Rings"},"/zerg/docs/architecture/connection-lifecycle/":{"data":{"":"A connection in zerg goes through a well-defined lifecycle: accept, register, use, close, and optionally return to pool.","accept-phase#Accept Phase":"The acceptor’s io_uring delivers a CQE with the new client fd TCP_NODELAY is set on the socket The fd is enqueued to the target reactor’s ConcurrentQueue","active-phase#Active Phase":"The connection is now active. The handler can:\nReadAsync() – park until data arrives, then drain ring items Write() – stage bytes into the unmanaged write slab FlushAsync() – tell the reactor to send staged bytes ResetRead() – prepare for the next read cycle See Connection Read and Connection Write for API details.","clear--safe-reset#\u003ccode\u003eClear()\u003c/code\u003e – Safe Reset":"Increments _generation to invalidate in-flight ValueTask tokens Publishes _closed = 1 Cancels any waiting read or flush waiter with OperationCanceledException Resets all write buffer state (WriteHead, WriteTail = 0) Resets both _readSignal and _flushSignal Clears the SPSC receive ring","clear2--fast-reset#\u003ccode\u003eClear2()\u003c/code\u003e – Fast Reset":"Increments _generation Publishes _closed = 1 Clears the receive ring and resets completion state Does not cancel waiters (assumes they’ve already exited) Faster than Clear() for hot-path pooling","close-phase#Close Phase":"A connection closes when:\nClient disconnects: recv CQE arrives with res == 0 (EOF) or res \u003c 0 (error) Ring overflow: the SPSC recv ring is full (1024 items) – the connection is force-closed as a safety measure Application closes: the handler exits the read loop When the reactor detects a close (recv CQE with res \u003c= 0):\nReturns any buffer used by the final CQE to the buffer ring Removes the connection from the reactor’s connections dictionary Marks the connection as closed (_closed = 1) Wakes any waiting ReadAsync() so the handler sees IsClosed == true","connection-object-layout#Connection Object Layout":"partial class Connection : IBufferWriter\u003cbyte\u003e, IValueTaskSource\u003cReadResult\u003e, IValueTaskSource, IDisposable { // Identity int ClientFd; Engine.Reactor Reactor; int _generation; // Read state SpscRecvRing _recv; // capacity: 1024 ManualResetValueTaskSourceCore\u003cReadResult\u003e _readSignal; int _armed, _pending, _closed; // Write state byte* WriteBuffer; // 64-byte aligned unmanaged slab int WriteHead, WriteTail, WriteInFlight; int SendInflight; // reactor-owned flag ManualResetValueTaskSourceCore\u003cbool\u003e _flushSignal; int _flushArmed, _flushInProgress; }","generation-counter#Generation Counter":"The _generation counter (incremented on every reuse) serves as the ValueTask token. If a stale ReadAsync() completes after the connection has been reused, GetResult() detects the mismatched token and returns ReadResult.Closed() instead of delivering stale data. This prevents use-after-free bugs in the async machinery.","lifecycle-stages#Lifecycle Stages":"Accept (Acceptor) │ ▼ ┌──────────────┐ │ Distribute │ round-robin to reactor │ via queue │ └──────┬───────┘ │ ▼ ┌──────────────┐ │ Register │ arm multishot recv │ in reactor │ add to connections dict └──────┬───────┘ │ ▼ ┌──────────────┐ │ Notify │ push to Channel │ application │ AcceptAsync() returns └──────┬───────┘ │ ▼ ┌──────────────┐ ┌──▶│ ReadAsync │◀─┐ │ │ + process │ │ read/write loop │ │ + Write │ │ │ │ + FlushAsync│──┘ │ └──────┬───────┘ │ │ │ ▼ (IsClosed or error) │ ┌──────────────┐ │ │ Close │ connection removed │ │ + cleanup │ from reactor dict │ └──────┬───────┘ │ │ │ ▼ │ ┌──────────────┐ └───│ Pool/Reuse │ Clear(), return to pool │ (optional) │ generation incremented └──────────────┘","pooling-and-reuse#Pooling and Reuse":"Connections can be pooled to avoid repeated allocation. The Connection class supports two reset methods:","registration-phase#Registration Phase":"On its next loop iteration, the reactor:\nDequeues the fd from its queue Creates or retrieves a Connection from the pool Calls connection.SetFd(clientFd).SetReactor(this) which: Assigns the file descriptor Clears the _closed flag Resets _pending and _armed flags Resets the _readSignal completion source Clears the SPSC receive ring Stores the connection in connections[clientFd] Arms multishot recv with buffer selection for the fd Pushes a ConnectionItem to the Channel for AcceptAsync()"},"title":"Connection Lifecycle"},"/zerg/docs/architecture/io-uring/":{"data":{"":"io_uring is a Linux kernel interface for asynchronous I/O. zerg uses it as its sole I/O mechanism – there are no epoll, kqueue, or libuv fallbacks.\nThe Ring Model","features-used-by-zerg#Features Used by zerg":"Multishot Accept A single SQE arms the kernel to produce one CQE per accepted connection indefinitely. The acceptor thread never re-arms. Each CQE contains the new client fd in cqe-\u003eres and IORING_CQE_F_MORE to indicate more will follow.\nMultishot Recv + Buffer Selection A single SQE arms recv for a connection. Each time data arrives, the kernel picks a buffer from the buf_ring, fills it, and produces a CQE with the buffer ID in the flags. Eliminates per-recv buffer allocation.\nBuffer Rings (Provided Buffers) Pre-allocated buffer pool registered with the kernel via shim_setup_buf_ring(). Buffers are added with buf_ring_add() and recycled after use. See Buffer Rings for the full lifecycle.\nSINGLE_ISSUER Tells the kernel only one thread submits to this ring. Skips SQ locking for better throughput. Matches zerg's model where each reactor is the sole submitter to its ring.\nDEFER_TASKRUN Defers kernel task_work until the next ring entry. Reduces latency spikes from interrupt-context work and makes completions arrive at predictable points for better async/await integration.\nSQPOLL (Optional) Creates a kernel thread polling the SQ continuously, eliminating the io_uring_enter() syscall. Trades a dedicated CPU core for the lowest possible submission latency.\nSubmit-and-Wait zerg's reactor uses shim_submit_and_wait_timeout() — a single syscall that submits all pending SQEs AND waits for at least one CQE. One syscall instead of two.\nCQE Batching Instead of one CQE at a time, the reactor peeks a batch with shim_peek_batch_cqe() and processes all before advancing the CQ head. Amortizes the head update across completions.","full-zerg-flow#Full zerg Flow":"Walk through the complete lifecycle: client connects, data flows in, your app responds, buffers recycle. Step through each phase one at a time.\nACCEPTOR THREAD Client TCP SYN connect Kernel multishot accept completes → CQE res = new client fd Accept CQE fd = 42 F_MORE = 1 (stay armed) Round Robin next % N Multishot accept is armed once at startup. Each new connection produces a CQE automatically. The acceptor never re-submits. F_MORE flag tells us the kernel will keep producing CQEs. ACCEPTOR REACTOR 0 Acceptor fd = 42 MPSC Queue lock-free enqueue Reactor dequeues fd creates Connection object Arm recv_multishot prep_recv(sqe, fd=42, bgid=1) + set_data64([Recv | 42]) The reactor now owns this fd. No other thread touches it. Zero contention by design. KERNEL I/O Client sends data Kernel recv() multishot → picks buf from buf_ring (bgid=1) Buffer #7 32KB pre-allocated filled: 1,420 bytes CQE produced user_data: [Recv | 42] res: 1420 flags: (7 \u003c\u003c 16) | F_BUFFER | F_MORE buffer_id=7 encoded in flags upper 16 bits • F_MORE means multishot continues Zero Copy NIC → your buffer REACTOR EVENT LOOP peek_batch_cqe got 3 CQEs Unpack user_data kind = UdKindOf(ud) fd = UdFdOf(ud) Recv extract buffer_id enqueue to connection Send update WriteHead += res complete flush if done Cancel cleanup acknowledged nothing to do CONNECTION READ PATH Recv CQE buf_id=7, 1420B ptr = slab + 7*32KB MPSC Recv Ring EnqueueRingItem (ptr, len, buf_id) ReadAsync() completes ValueTask signals your handler result.IsClosed = false zero allocation (ManualResetValueTaskSourceCore) The reactor enqueues the raw pointer + length + buffer_id into the connection's lock-free recv ring. Your await ReadAsync() resumes with zero allocations via ValueTask. You now have direct pointer access to the kernel-filled buffer. No copy happened. YOUR APPLICATION HANDLER Read Buffers GetAllSnapshot RingsAsUnmanagedMemory or ToReadOnlySequence() Parse Request your business logic HTTP parse, JSON, etc. Write Response connection.Write(data) copies into 16KB staging slab NativeMemory, no GC pressure await FlushAsync() queues send SQE for reactor Write() copies into the unmanaged staging buffer. FlushAsync() signals the reactor to submit a send SQE on the next loop iteration. REACTOR KERNEL Drain Flush Q prep_send(fd, buf, len) submit_and _wait_timeout 1 syscall Kernel send() staging buf → TCP socket → client receives data Send CQE res = bytes_sent reactor: CompleteFlush() FlushAsync returns your handler continues BUFFER RECYCLING Your Handler ReturnRing(bid=7) ResetRead() Return Queue MPSC enqueue(7) Reactor drains DrainReturnQ() on next loop iter buf_ring_add + buf_ring_advance buffer #7 is back in the pool kernel can use it for the next recv await ReadAsync() ready for next request ← Loop repeats. Zero allocations. 1 syscall per iteration. → ◀ ▶","how-io_uring-works#How io_uring Works":"io_uring uses two lock-free ring buffers shared between userspace and the kernel. Your app writes SQEs (requests), the kernel writes CQEs (results). No syscall needed for submission.\nUSERSPACE (your C# app) KERNEL SHARED MEMORY (mmap'd) Submission Queue SQE SQE SQE Completion Queue CQE CQE write SQEs kernel reads SQ kernel writes CQ read CQEs I/O Processing accept / recv / send Your Code prep_recv, prep_send... Your Handler process results, route... SQE Structure (Submission) opcode fd buf/len user_data flags CQE Structure (Completion) user_data res flags SQE — Submission Queue Entry opcode = what to do (recv, send, accept)\nfd = which socket\nuser_data = your 64-bit tag (returned in CQE)\nflags = BUFFER_SELECT, etc.\nCQE — Completion Queue Entry user_data = your original tag (identifies the op)\nres = result (bytes transferred, new fd, or -errno)\nflags = MORE, BUFFER (contains buffer_id)\nShared Memory Both rings are mmap'd. The kernel and your app write to them directly. No copy, no syscall for enqueue. Only io_uring_enter() needed to wake the kernel.\nInteractive","multishot-operations#Multishot Operations":"Traditional I/O: 1 SQE → 1 CQE. Multishot: 1 SQE → many CQEs. The kernel keeps producing completions until an error or you cancel.\nTraditional (one-shot) Submit recv for each read SQE recv CQE data SQE recv CQE data SQE recv CQE data SQE recv CQE data 4 SQEs 4 submissions 4 completions Cost: re-arm after every read More SQE slots consumed More CPU cycles on submission Multishot (zerg) Submit once, get many completions 1 SQE recv_multishot CQE + MORE CQE + MORE CQE + MORE CQE final F_MORE = 1 F_MORE = 1 F_MORE = 1 F_MORE = 0 Win: 1 submission, N completions Kernel sets IORING_CQE_F_MORE on each CQE When MORE=0 → multishot ended, re-arm Used for both accept and recv in zerg","provided-buffer-ring#Provided Buffer Ring":"Instead of passing a buffer with each recv, you pre-register a pool. The kernel picks one, fills it, and tells you which ID it used. You return it when done.\nUSERSPACE KERNEL Buffer Slab (NativeMemory) buf 0 32KB buf 1 32KB buf 2 32KB buf 3 32KB ... buf N 32KB Buffer Ring (shared with kernel) id:0 id:1 id:2 used id:3 id:4 id:5 ... id:N Recv Flow 1 Kernel picks buf from ring 2 recv() fills it with data 3 CQE.flags contains buf id bid = flags \u003e\u003e 16 4 CQE.res = bytes received 5 App returns buf via buf_ring_add Kernel: recv(fd) → picks buf 2 → fills 1,420 bytes CQE { user_data: [Recv|fd], res: 1420, flags: (2 \u003c\u003c 16) | F_BUFFER | F_MORE } Buffer Return (app → ring) connection.ReturnRing(bid) → MPSC queue → reactor drains → shim_buf_ring_add(ring, addr, len, bid, mask, idx) → shim_buf_ring_advance(1) End to End","the-io-lifecycle#The I/O Lifecycle":"Step through the exact sequence: your app queues an SQE, the kernel processes it, and you read the CQE result. All in shared memory.\nUSERSPACE ━━ kernel boundary ━━ KERNEL get_sqe() grab empty SQE slot prep_recv(sqe, fd) fill opcode + fd + flags set_data64(sqe, tag) attach your 64-bit token submit() io_uring_enter Kernel processes I/O recv(fd) → data into buffer CQE written to CQ user_data + res + flags App reads CQE dispatch by user_data cq_advance(count) mark CQEs consumed ◀ ▶ zerg — single syscall pattern // 1. Queue work (no syscall) io_uring_sqe* sqe = shim_get_sqe(ring); shim_prep_recv_multishot_select(sqe, fd, bgid, 0); shim_sqe_set_data64(sqe, PackUd(UdKind.Recv, fd)); // 2. Submit + wait in ONE syscall shim_submit_and_wait_timeout(ring, \u0026cqes, 1, \u0026ts);\n// 3. Batch-read completions (no syscall) int got = shim_peek_batch_cqe(ring, cqes, batchSize);\n// 4. Process results for (int i = 0; i \u003c got; i++) { UdKind kind = UdKindOf(shim_cqe_get_data64(cqes[i])); int res = cqes[i]-\u003eres; // dispatch… } shim_cq_advance(ring, (uint)got); // 5. Mark consumed\nKey Feature","user_data-packing#user_data Packing":"Each SQE carries a 64-bit token so the completion handler knows what operation completed and on which socket.\n64-bit user_data UdKind (bits 63-32) 1=Accept 2=Recv 3=Send 4=Cancel File Descriptor (bits 31-0) socket fd cast to uint PackUd(kind, fd) = ((ulong)kind \u003c\u003c 32) | (uint)fd Zero Copy"},"title":"io_uring"},"/zerg/docs/architecture/reactor-pattern/":{"data":{"":"zerg implements a classic reactor pattern with a split-architecture design: one dedicated acceptor thread and N independent reactor threads.","acceptor-event-loop#Acceptor Event Loop":"loop: cqeCount = peek_batch_cqe(ring, cqes, batchSize) if cqeCount == 0: submit_and_wait_timeout(ring, timeout) continue for each cqe in cqes: if cqe.res \u003c 0: log error, continue clientFd = cqe.res setsockopt(clientFd, TCP_NODELAY) reactorQueues[nextReactor++ % reactorCount].Enqueue(clientFd) cq_advance(ring, cqeCount)","acceptor-thread#Acceptor Thread":"The acceptor is responsible for one job: accepting new TCP connections.\nCreates a listening socket (IPv4 or IPv6 dual-stack, configurable via IPVersion) Binds and listens with the configured Backlog Sets up its own io_uring and arms a multishot accept SQE Enters an event loop that: Peeks a batch of CQEs (accepted file descriptors) Sets TCP_NODELAY on each accepted socket Distributes fds to reactors in round-robin order via lock-free ConcurrentQueue (one per reactor) Sleeps in io_uring_wait_cqes() when idle Multishot accept means a single submission produces a CQE for every incoming connection without re-arming. The acceptor never allocates per-connection – it just hands off integer file descriptors.","application-integration#Application Integration":"After a reactor registers a new connection, it pushes a ConnectionItem (reactor ID + client fd) into an unbounded Channel. The Engine.AcceptAsync() method reads from this channel, returning fully-registered Connection objects to the application.\nThis means by the time your handler receives a connection:\nThe connection is already assigned to a reactor Multishot recv is already armed The buffer ring is ready to receive data You can immediately call ReadAsync()","connection-distribution#Connection Distribution":"The acceptor distributes connections using a simple round-robin counter:\nreactorIndex = acceptCount++ % reactorCount Each reactor gets approximately equal load. The distribution is via ConcurrentQueue – one queue per reactor – so the acceptor never blocks waiting for a reactor.","overview#Overview":"Kernel Space C1 C2 C3 C4 C5 ... TCP ↓ Listening Socket bind + listen • backlog queue User Space Acceptor Thread Single SQE → multishot accept → one CQE per connection io_uring TCP_NODELAY round-robin fd distribution ↓ ↓ ↓ ConcurrentQueue ConcurrentQueue ConcurrentQueue Reactor 0 io_uring buf_ring SPSC ring conn_map flush_Q multishot recv + send Reactor 1 io_uring buf_ring SPSC ring conn_map flush_Q multishot recv + send Reactor N io_uring buf_ring SPSC ring conn_map flush_Q multishot recv + send Channel Application Handlers Engine.AcceptAsync() → ReadAsync ↔ Write + FlushAsync Every thread in the system owns its own io_uring instance. There is no shared ring, and no lock contention on the I/O path.","reactor-event-loop#Reactor Event Loop":"Each reactor runs a tight loop:\nloop: // 1. Drain newly accepted connections while reactorQueue.TryDequeue(out clientFd): connection = pool.Get() or new Connection() connection.SetFd(clientFd).SetReactor(this) connections[clientFd] = connection arm multishot_recv_select(clientFd, bufferGroupId) notify application via Channel // 2. Drain buffer returns while returnQ.TryDequeue(out bufferId): buf_ring_add(bufferRing, slab + bufferId * bufSize, bufSize, bufferId, mask, idx++) buf_ring_advance(bufferRing, returnCount) // 3. Drain flush requests while flushQ.TryDequeue(out flushFd): connection = connections[flushFd] prep_send(sqe, flushFd, connection.WriteBuffer, connection.WriteInFlight, 0) submit pending sends // 4. Process completions cqeCount = peek_batch_cqe(ring, cqes, batchSize) for each cqe: kind = UdKindOf(cqe.user_data) fd = UdFdOf(cqe.user_data) if kind == Recv: if cqe.res \u003c= 0: close connection, return buffer else: enqueue RingItem to connection, wake handler if kind == Send: advance WriteHead, resubmit if partial, signal flush complete if kind == Cancel: handle cancellation completion cq_advance(ring, cqeCount) submit_and_wait_timeout(ring, timeout)","reactor-threads#Reactor Threads":"Each reactor thread owns:\nIts own io_uring instance (created with SINGLE_ISSUER | DEFER_TASKRUN by default) A buffer ring for zero-copy receive operations A Dictionary mapping file descriptors to connection objects Lock-free queues for receiving new fds from the acceptor and flush requests from handlers"},"title":"Reactor Pattern"},"/zerg/docs/architecture/threading-model/":{"data":{"":"zerg uses a fixed set of dedicated threads with strict ownership rules. Every piece of mutable state is owned by exactly one thread, and all cross-thread communication happens through lock-free queues with well-defined memory ordering.","acceptor--reactor-new-connections#Acceptor → Reactor: New Connections":"ConcurrentQueue ReactorQueues[reactorId] The acceptor enqueues integer file descriptors. Each reactor drains its queue at the start of every loop iteration. .NET ConcurrentQueue provides full thread-safety.","cpu-affinity#CPU Affinity":"zerg provides optional CPU pinning for reactor threads via the Affinity class:\nAffinity.PinCurrentThreadToCpu(cpuId); This uses the Linux sched_setaffinity syscall to bind a thread to a specific core. Pinning prevents the OS scheduler from migrating threads, which improves cache locality and reduces jitter.\nCPU pinning is optional and best-effort – if it fails (e.g., in containers with CPU limits), the thread continues on whatever core the scheduler assigns.","cross-thread-communication#Cross-Thread Communication":"All cross-thread data flow uses lock-free queues:","handler--reactor-buffer-returns#Handler → Reactor: Buffer Returns":"MpscUshortQueue returnQ When a handler calls connection.ReturnRing(bufferId), the ushort buffer ID is enqueued to the reactor’s MPSC return queue. The reactor drains this queue and returns buffers to the kernel buffer ring.","handler--reactor-flush-requests#Handler → Reactor: Flush Requests":"MpscIntQueue flushQ When a handler calls FlushAsync(), the connection’s client fd is enqueued to the reactor’s flush queue. The reactor drains this and issues send SQEs.","handler-task-execution#Handler Task Execution":"Handler tasks (_ = HandleConnectionAsync(connection)) run on the .NET thread pool. When a handler awaits ReadAsync() or FlushAsync():\nThe handler task yields back to the thread pool The reactor thread completes the ValueTask source when data/flush is ready The handler resumes on a thread pool thread (not the reactor thread) This means:\nReactor threads never execute user handler code Handlers never block reactor threads Multiple handlers can be active simultaneously per reactor DEFER_TASKRUN ensures completions arrive at predictable points in the reactor loop","interlocked-full-fence#Interlocked (Full Fence)":"Used in MPSC queues where multiple producers contend:\nInterlocked.CompareExchange(ref _armed, 0, 1); // atomically disarm Interlocked.Increment(ref _tail); // reserve MPSC slot","memory-ordering#Memory Ordering":"zerg uses three levels of memory ordering:","ownership-rules#Ownership Rules":"State Owner Accessed By Acceptor io_uring Acceptor thread Acceptor only Reactor io_uring Reactor thread Reactor only Buffer ring Reactor thread Reactor (add/advance), handler (ReturnRing via MPSC queue) connections dict Reactor thread Reactor only Connection read state (_recv, _armed, etc.) Split Reactor produces, handler consumes Connection write slab Handler Handler writes, reactor reads during flush SendInflight flag Reactor thread Reactor writes (Volatile), handler reads","plain-reads-single-consumer#Plain Reads (Single-Consumer)":"The consumer side of SPSC/MPSC queues uses plain reads for _head since only one thread reads and writes it:\nvar head = _head; // only consumer writes _head _head = head + 1; // safe: single writer","reactor--handler-flush-completion#Reactor → Handler: Flush Completion":"ManualResetValueTaskSourceCore _flushSignal int _flushArmed, _flushInProgress When all staged bytes are sent, the reactor completes the flush signal, resuming the handler’s await FlushAsync().","reactor--handler-read-completion#Reactor → Handler: Read Completion":"SpscRecvRing _recv (per connection) int _armed, _pending (atomics) ManualResetValueTaskSourceCore _readSignal The reactor enqueues RingItems to the connection’s SPSC ring and wakes the handler via the ValueTask completion source.","thread-count#Thread Count":"Total threads = 1 (acceptor) + N (reactors), where N = EngineOptions.ReactorCount.\nHandler tasks run on the .NET thread pool and are not dedicated threads. Multiple handler tasks may be active per reactor, but each connection’s ReadAsync/FlushAsync enforces single-waiter semantics.","thread-layout#Thread Layout":"┌─────────────────┐ │ Acceptor Thread │ 1 thread │ (io_uring) │ Accepts connections, distributes fds └────────┬────────┘ │ ConcurrentQueue per reactor ▼ ┌───────────────┐ ┌───────────────┐ ┌───────────────┐ │ Reactor 0 │ │ Reactor 1 │ │ Reactor N │ N threads │ (io_uring) │ │ (io_uring) │ │ (io_uring) │ Event loops │ (buf_ring) │ │ (buf_ring) │ │ (buf_ring) │ │ (conn dict) │ │ (conn dict) │ │ (conn dict) │ └───────┬───────┘ └───────┬───────┘ └───────┬───────┘ │ │ │ ▼ ▼ ▼ ┌───────────────┐ ┌───────────────┐ ┌───────────────┐ │ Handler Tasks│ │ Handler Tasks│ │ Handler Tasks│ Task pool │ (async/await)│ │ (async/await)│ │ (async/await)│ User code └───────────────┘ └───────────────┘ └───────────────┘","volatile-readwrite#Volatile Read/Write":"Used for single-word flags where only visibility is needed:\nVolatile.Write(ref _closed, 1); // publish close Volatile.Read(ref _pending); // check pending flag Volatile.Write(ref SendInflight, 0); // clear in-flight flag"},"title":"Threading Model"},"/zerg/docs/getting-started/":{"data":{"":"This section walks you through installing zerg, writing your first server, and understanding the configuration options available.\nInstallationNuGet, source build, native dependencies, and AOT Quick StartA minimal TCP server with connection handling ConfigurationEngineOptions, ReactorConfig, and AcceptorConfig reference"},"title":"Getting Started"},"/zerg/docs/getting-started/configuration/":{"data":{"":"zerg is configured through three types: EngineOptions (top-level), ReactorConfig (per-reactor), and AcceptorConfig (acceptor thread). All have sensible defaults so you can start with just a port number.","acceptorconfig#AcceptorConfig":"Configuration for the acceptor thread’s io_uring instance. This is a sealed record.\nProperty Type Default Description RingFlags uint 0 io_uring setup flags for the acceptor ring. SQPOLL is usually unnecessary here. SqCpuThread int -1 CPU core for SQPOLL thread. SqThreadIdleMs uint 100 SQPOLL idle timeout in milliseconds. RingEntries uint 8192 SQ/CQ size. Bounds in-flight accept completions. BatchSqes uint 4096 Max accepts processed per loop iteration. CqTimeout long 100_000_000 (100 ms) Wait timeout in nanoseconds. Higher than reactor default since accepts are burst-driven. IPVersion IPVersion IPv6DualStack IP stack for the listening socket.","engineoptions#EngineOptions":"The top-level configuration passed to new Engine(options).\nProperty Type Default Description ReactorCount int 1 Number of reactor threads to spawn. Each gets its own io_uring instance. Ip string \"0.0.0.0\" IP address to bind the listening socket to. Use \"::\" for IPv6. Port ushort 8080 TCP port to listen on. Backlog int 65535 Kernel listen backlog for pending connections. AcceptorConfig AcceptorConfig new() Configuration for the acceptor ring and event loop. ReactorConfigs ReactorConfig[] null Per-reactor configuration array. Auto-initialized with defaults if null. Must have at least ReactorCount entries if provided.","example#Example":"var engine = new Engine(new EngineOptions { Ip = \"0.0.0.0\", Port = 8080, ReactorCount = 12, Backlog = 65535 });","example-custom-acceptor#Example: Custom Acceptor":"var engine = new Engine(new EngineOptions { Port = 443, ReactorCount = 8, AcceptorConfig = new AcceptorConfig( RingEntries: 16384, CqTimeout: 50_000_000, // 50 ms IPVersion: IPVersion.IPv4Only ) });","example-per-reactor-configuration#Example: Per-Reactor Configuration":"var engine = new Engine(new EngineOptions { Port = 8080, ReactorCount = 4, ReactorConfigs = Enumerable.Range(0, 4).Select(_ =\u003e new ReactorConfig( RecvBufferSize: 64 * 1024, // 64 KB recv buffers BufferRingEntries: 32 * 1024, // 32K buffers per reactor CqTimeout: 500_000 // 0.5 ms timeout )).ToArray() });","ipversion-enum#IPVersion Enum":"Value Description IPv4Only Creates AF_INET socket. Only IPv4 clients can connect. IPv6DualStack Creates AF_INET6 socket with IPV6_V6ONLY=0. Accepts both IPv4 and IPv6 clients. IPv4 clients appear as ::ffff:a.b.c.d.","memory-budget#Memory Budget":"A rough estimate of per-reactor memory usage:\nComponent Formula Default Buffer ring BufferRingEntries * RecvBufferSize 16384 * 32 KB = 512 MB Write slabs MaxConnectionsPerReactor * 16 KB 8192 * 16 KB = 128 MB Ring entries RingEntries * sizeof(SQE/CQE) ~1 MB For a 4-reactor server with defaults, the buffer ring alone accounts for ~2 GB. Adjust BufferRingEntries and RecvBufferSize based on your workload and available memory.","reactorconfig#ReactorConfig":"Configuration for a single reactor’s io_uring instance and event loop. This is a sealed record – all properties have defaults.\nProperty Type Default Description RingFlags uint SINGLE_ISSUER | DEFER_TASKRUN io_uring setup flags. See Ring Flags below. SqCpuThread int -1 CPU core to pin the SQPOLL kernel thread to. Only used with IORING_SETUP_SQPOLL. -1 = kernel decides. SqThreadIdleMs uint 100 How long (ms) the SQPOLL kernel thread stays alive without submissions before sleeping. RingEntries uint 8192 SQ/CQ size. Upper bound on in-flight operations (recv, send, cancel) the reactor can have at once. RecvBufferSize int 32768 (32 KB) Size of each receive buffer in the buffer ring. Larger values reduce syscalls for large payloads. BufferRingEntries int 16384 Number of pre-allocated recv buffers. Must be a power of two. BatchCqes int 4096 Maximum CQEs processed per loop iteration. Larger improves throughput under load. MaxConnectionsPerReactor int 8192 Upper bound on concurrent connections. Should be \u003c= RingEntries. CqTimeout long 1_000_000 (1 ms) Timeout in nanoseconds passed to io_uring_wait_cqes(). Lower = lower tail latency, higher CPU.","ring-flags#Ring Flags":"io_uring setup flags control how the kernel processes submissions and completions. You can combine flags with bitwise OR.\nFlag Value Description IORING_SETUP_SQPOLL 1 \u003c\u003c 1 Kernel thread polls the SQ, eliminating submit syscalls. Trades a CPU core for lower latency. IORING_SETUP_SQ_AFF 1 \u003c\u003c 2 Pin the SQPOLL kernel thread to the CPU specified by SqCpuThread. IORING_SETUP_SINGLE_ISSUER 1 \u003c\u003c 12 Optimize for a single submitting thread. Default for reactors. IORING_SETUP_DEFER_TASKRUN 1 \u003c\u003c 13 Defer kernel task_work execution, reducing latency spikes. Default for reactors. IORING_SETUP_CQSIZE 1 \u003c\u003c 3 Allow CQ size different from SQ size. IORING_SETUP_CLAMP 1 \u003c\u003c 4 Clamp queue sizes to kernel-supported limits.","sqpoll-example#SQPOLL Example":"// Enable SQPOLL on reactor 0, pinned to CPU 2 var config = new ReactorConfig( RingFlags: ABI.IORING_SETUP_SQPOLL | ABI.IORING_SETUP_SQ_AFF | ABI.IORING_SETUP_SINGLE_ISSUER, SqCpuThread: 2, SqThreadIdleMs: 200 );"},"title":"Configuration"},"/zerg/docs/getting-started/installation/":{"data":{"":"","aot-compilation#AOT Compilation":"zerg is compatible with Native AOT compilation. The native liburingshim.so P/Invoke bindings use static linking-friendly signatures. To publish with AOT:\ndotnet publish -c Release -r linux-x64 /p:PublishAot=true","build-from-source#Build from Source":"Clone the repository and build with the .NET SDK:\ngit clone https://github.com/MDA2AV/uRocket.git cd uRocket dotnet build The solution file zerg.sln includes the core library, examples, playground, and TechEmpower benchmark projects.","kernel-requirements#Kernel Requirements":"zerg requires a Linux kernel with io_uring support:\nMinimum: Linux 5.10+ (basic io_uring operations) Recommended: Linux 6.0+ (multishot accept, buffer rings, IORING_SETUP_SINGLE_ISSUER) Optimal: Linux 6.1+ (IORING_SETUP_DEFER_TASKRUN) You can check your kernel version with:\nuname -r","managed-dependency#Managed Dependency":"zerg has a single managed dependency:\nMicrosoft.Extensions.ObjectPool (v10.0.2) – used for connection pooling","native-dependencies#Native Dependencies":"zerg ships with bundled native libraries for the io_uring shim:\nRuntime Library linux-x64 (glibc) liburingshim.so linux-musl-x64 (Alpine) liburingshim.so These are included in the NuGet package and copied to your output directory automatically. No manual installation of liburing is required.","nuget-package#NuGet Package":"The recommended way to install zerg is via NuGet:\ndotnet add package zerg Or add the package reference directly to your .csproj:","project-structure#Project Structure":"When you reference zerg, the following projects are available in the solution:\nProject Description zerg Core library (the NuGet package) Examples Basic usage examples (ReadOnlySpan, ReadOnlySequence) Playground Development sandbox TechEmpower/BenchmarkApp HTTP/1.1 benchmark server"},"title":"Installation"},"/zerg/docs/getting-started/quick-start/":{"data":{"":"This guide shows you how to create a minimal TCP server with zerg, accept connections, read data, and send a response.","connection-handler#Connection Handler":"Each accepted connection gives you a Connection object. The read/write cycle follows this pattern:\nReadAsync – wait for inbound data Process – inspect received buffers ReturnRing – return kernel buffers after processing Write – stage response bytes FlushAsync – send staged bytes to the kernel ResetRead – prepare for the next read cycle static async Task HandleConnectionAsync(Connection connection) { while (true) { // Wait for data from the client var result = await connection.ReadAsync(); if (result.IsClosed) break; // Get all received buffers as managed memory var rings = connection.GetAllSnapshotRingsAsUnmanagedMemory(result); // Build a ReadOnlySequence for parsing ReadOnlySequence\u003cbyte\u003e sequence = rings.ToReadOnlySequence(); // ... parse the request from sequence ... // Return buffers to the kernel buffer ring rings.ReturnRingBuffers(connection.Reactor); // Write a response connection.Write(\"HTTP/1.1 200 OK\\r\\nContent-Length: 13\\r\\nContent-Type: text/plain\\r\\n\\r\\nHello, World!\"u8); // Flush to kernel and wait for send completion await connection.FlushAsync(); // Ready for the next read connection.ResetRead(); } }","minimal-server#Minimal Server":"using zerg.Engine; using zerg.Engine.Configs; // 1. Create the engine with configuration var engine = new Engine(new EngineOptions { Port = 8080, ReactorCount = Environment.ProcessorCount }); // 2. Start listening -- spawns acceptor + reactor threads engine.Listen(); // 3. Graceful shutdown on Enter key var cts = new CancellationTokenSource(); _ = Task.Run(() =\u003e { Console.ReadLine(); engine.Stop(); cts.Cancel(); }); // 4. Accept loop try { while (engine.ServerRunning) { var connection = await engine.AcceptAsync(cts.Token); if (connection is null) continue; // Fire-and-forget handler per connection _ = HandleConnectionAsync(connection); } } catch (OperationCanceledException) { Console.WriteLine(\"Server stopped.\"); }","next-steps#Next Steps":"Configuration – tune buffer sizes, reactor count, and ring flags Architecture – understand the acceptor + reactor model Zero-Allocation Guide – allocation-free read and write patterns","what-happens-under-the-hood#What Happens Under the Hood":"When you call engine.Listen():\nAn acceptor thread starts with its own io_uring instance and arms multishot accept on the listening socket N reactor threads start, each with their own io_uring instance and pre-allocated buffer ring As clients connect, the acceptor distributes file descriptors to reactors in round-robin order Each reactor arms multishot recv with buffer selection for its connections AcceptAsync returns connections as they are fully registered in a reactor When you call connection.ReadAsync():\nIf data is already queued in the connection’s SPSC ring, it returns immediately Otherwise, the calling task parks until the reactor delivers data via a CQE completion The returned ReadResult contains a snapshot boundary so you drain exactly the data that was available at that point When you call connection.FlushAsync():\nThe write tail is captured as the flush target The connection is enqueued to the reactor’s flush queue The reactor issues a send SQE and handles partial sends automatically The ValueTask completes when all staged bytes have been sent"},"title":"Quick Start"},"/zerg/docs/guides/":{"data":{"":"Practical guides for building applications with zerg.\nZero-Allocation PatternsAllocation-free read and write strategies Buffer ManagementBuffer lifecycle, preventing leaks, ownership rules Performance TuningTunables, SQPOLL, sizing, and benchmarking"},"title":"Guides"},"/zerg/docs/guides/buffer-management/":{"data":{"":"Correct buffer management is critical in zerg. Kernel-provided buffers are a finite pool – leaking them will stall your server. This guide covers the buffer lifecycle, ownership rules, and common pitfalls.","buffer-ownership-model#Buffer Ownership Model":"Every buffer in zerg has exactly one owner at any time:\nKernel (buffer ring) → Reactor (CQE) → Connection (SPSC ring) → Handler → Return → Kernel State Owner What You Can Do In buffer ring Kernel Nothing – kernel picks buffers for recv CQE delivered Reactor Reactor creates RingItem, enqueues to connection In SPSC ring Connection Waiting to be drained by handler Dequeued Handler Read Ptr/AsSpan(). Must return when done. Returned Kernel Buffer back in pool. All pointers are invalid.","buffer-pool-exhaustion#Buffer Pool Exhaustion":"If all buffers are in-flight (held by the kernel or handlers):\nNew recv operations have no buffers to use Multishot recv CQEs may stop arriving for affected connections If the connection’s SPSC ring fills (1024 items), the connection is force-closed","common-mistakes#Common Mistakes":"","forgetting-to-return-on-error#Forgetting to Return on Error":"// BAD: buffer leaked on exception while (connection.TryGetRing(result.TailSnapshot, out RingItem ring)) { ProcessData(ring.AsSpan()); // throws! connection.ReturnRing(ring.BufferId); // never reached } // GOOD: return in finally while (connection.TryGetRing(result.TailSnapshot, out RingItem ring)) { try { ProcessData(ring.AsSpan()); } finally { connection.ReturnRing(ring.BufferId); } }","holding-buffers-too-long#Holding Buffers Too Long":"// BAD: holding buffer during async operation while (connection.TryGetRing(result.TailSnapshot, out RingItem ring)) { await SomeDatabaseCallAsync(ring.AsSpan()); // buffer held during I/O connection.ReturnRing(ring.BufferId); } // GOOD: copy what you need, return immediately while (connection.TryGetRing(result.TailSnapshot, out RingItem ring)) { var requestData = ring.AsSpan().ToArray(); // or parse inline connection.ReturnRing(ring.BufferId); // return immediately await SomeDatabaseCallAsync(requestData); }","memory-layout-summary#Memory Layout Summary":"Per-Reactor: Buffer Ring Slab: BufferRingEntries * RecvBufferSize bytes (unmanaged, aligned) Buffer Ring: BufferRingEntries * sizeof(entry) (kernel-managed) Per-Connection: Write Slab: 16 KB default (unmanaged, 64-byte aligned) SPSC Recv Ring: 1024 * sizeof(RingItem) (managed array)","monitoring#Monitoring":"// Per-connection ring fill level long pending = connection.TotalRingCount; // After ReadAsync int batchSize = connection.SnapshotRingCount;","prevention#Prevention":"Return buffers promptly – don’t hold them across awaits Size BufferRingEntries appropriately – more buffers = more headroom Process data inline – parse and respond in the same sync block when possible Copy if you need to hold data – copy to managed/unmanaged memory, return the buffer","return-path-internals#Return Path Internals":"When you call connection.ReturnRing(bufferId):\nThe ushort buffer ID is enqueued to the reactor’s MPSC return queue On the reactor’s next loop iteration, it drains the return queue For each returned buffer ID: shim_buf_ring_add(br, slab + bufferId * bufSize, bufSize, bufferId, mask, idx) After processing all returns: shim_buf_ring_advance(br, count) The kernel can now use these buffers for future recv operations The return is not immediate – there’s a small delay (one reactor loop iteration) before the kernel can reuse the buffer. This is safe because the buffer ring has thousands of buffers.","the-golden-rule#The Golden Rule":"Every buffer obtained from the connection must be returned exactly once.\n// Correct: obtain and return while (connection.TryGetRing(result.TailSnapshot, out RingItem ring)) { ProcessData(ring.AsSpan()); connection.ReturnRing(ring.BufferId); // MUST call this } // Also correct: batch obtain and return var rings = connection.GetAllSnapshotRingsAsUnmanagedMemory(result); // ... process ... rings.ReturnRingBuffers(connection.Reactor); // returns all at once","using-a-buffer-after-return#Using a Buffer After Return":"// BAD: use-after-return connection.TryGetRing(result.TailSnapshot, out RingItem ring); connection.ReturnRing(ring.BufferId); var data = ring.AsSpan(); // UNSAFE: kernel may have reused the buffer // GOOD: copy first if needed connection.TryGetRing(result.TailSnapshot, out RingItem ring); byte[] copy = ring.AsSpan().ToArray(); // copy to managed memory connection.ReturnRing(ring.BufferId); ProcessLater(copy); // safe: data is in managed memory","write-buffer-management#Write Buffer Management":"The write path uses a separate unmanaged slab per connection (default 16 KB):\nWrite(data) → copies into slab → FlushAsync() → reactor sends → slab reset The write slab is automatically reset after FlushAsync() completes. You don’t need to manage it manually.\nSize limit: The total staged data (all Write() calls between flushes) must fit in the slab. If your responses are larger than 16 KB, increase the slab size in the Connection constructor.\nFlush barrier: While a flush is in progress (_flushInProgress == 1), all Write() calls throw InvalidOperationException. Wait for the flush to complete before writing again."},"title":"Buffer Management"},"/zerg/docs/guides/performance-tuning/":{"data":{"":"This guide covers the key tunables in zerg and how to adjust them for your workload.","batch-cqes#Batch CQEs":"BatchCqes = 4096 Maximum CQEs processed per loop iteration. Larger values improve throughput under load by amortizing loop overhead, but increase per-loop latency (time to service all CQEs before sleeping again).\nFor latency-sensitive applications, consider reducing this to 256-1024.","benchmarking-tips#Benchmarking Tips":"Warm up – run at least 10 seconds of load before measuring Pin cores – use CPU affinity to prevent migration Disable turbo boost – for consistent results, disable CPU frequency scaling Use wrk or h2load – standard HTTP benchmarking tools Watch for kernel limits – check ulimit -n (file descriptor limit) and net.core.somaxconn Profile with perf – perf top shows where CPU time is spent","buffer-ring-sizing#Buffer Ring Sizing":"","bufferringentries#BufferRingEntries":"BufferRingEntries = 16 * 1024 // 16384 buffers Total receive buffers available to the kernel per reactor. Must be a power of two.\nSize it based on:\nMaxConnectionsPerReactor – at minimum, one buffer per active connection Data holding time – if handlers hold buffers during async work, you need more Burst capacity – buffers to absorb data bursts without stalling Memory impact: BufferRingEntries * RecvBufferSize per reactor.","configuration-matrix#Configuration Matrix":"Scenario ReactorCount RecvBufferSize BufferRingEntries CqTimeout Low-latency API CPU count 4 KB 8192 100,000 ns HTTP server CPU count 32 KB 16384 1,000,000 ns Proxy/gateway CPU count 64 KB 32768 500,000 ns File transfer CPU count / 2 128 KB 4096 10,000,000 ns IoT/many connections CPU count 2 KB 65536 1,000,000 ns","connection-limits#Connection Limits":"MaxConnectionsPerReactor = 8192 Upper bound on concurrent connections per reactor. This is a logical limit, not a hard allocation.\nScaling formula:\nTotal concurrent connections = ReactorCount * MaxConnectionsPerReactor With defaults: 1 * 8192 = 8,192 connections With 12 reactors: 12 * 8192 = 98,304 connections Ensure MaxConnectionsPerReactor \u003c= RingEntries to avoid SQE exhaustion.","cq-timeout#CQ Timeout":"CqTimeout = 1_000_000 // nanoseconds (1 ms) The CQ timeout controls how long the reactor sleeps when no completions are available.\nValue Tail Latency CPU Usage 100_000 (0.1 ms) Very low High (more frequent wakeups) 1_000_000 (1 ms) Low Moderate (default) 10_000_000 (10 ms) Moderate Low 100_000_000 (100 ms) High Very low For latency-sensitive servers, use 0.1-1 ms. For background or batch-oriented servers, 10-100 ms is fine.\nThe acceptor uses 100 ms by default since accept bursts are infrequent.","defer_taskrun#DEFER_TASKRUN":"RingFlags = ABI.IORING_SETUP_SINGLE_ISSUER | ABI.IORING_SETUP_DEFER_TASKRUN This is the default for reactors. It tells the kernel to defer task_work (completion callbacks) until the next io_uring_enter() call, rather than running them in interrupt context.\nBenefits:\nCompletions arrive at predictable points in the reactor loop Reduces latency spikes from interrupt-context work Better cache behavior When to disable: Rarely. Only if you’re seeing issues with specific kernel versions.","listen-backlog#Listen Backlog":"Backlog = 65535 Kernel queue for pending connections (accepted by kernel but not yet accepted by userspace). 65535 is the Linux maximum. Reduce only if you want to reject connections under load.","reactor-count#Reactor Count":"ReactorCount = Environment.ProcessorCount Rule of thumb: Start with one reactor per CPU core. Each reactor runs a tight event loop on a dedicated thread, so more reactors than cores leads to context switching overhead.\nFor mixed workloads (some connections do async I/O like database calls), you may benefit from slightly more reactors than cores since handler tasks yield to the thread pool during awaits.","recvbuffersize#RecvBufferSize":"RecvBufferSize = 32 * 1024 // 32 KB per buffer Each kernel recv writes into one of these buffers. If a recv delivers more data than the buffer size, the kernel fills the buffer and the remainder arrives in the next CQE.\nWorkload Recommended Size Small messages (HTTP/1.1 requests) 4-8 KB Mixed traffic 16-32 KB (default) Large uploads/downloads 64-128 KB Websockets with large frames 64+ KB","ring-entries#Ring Entries":"RingEntries = 8192 The SQ/CQ size. This is the maximum number of in-flight I/O operations:\nOne multishot recv per active connection One send per flushing connection Cancel operations Should be \u003e= MaxConnectionsPerReactor to avoid running out of SQE slots.","sqpoll#SQPOLL":"SQPOLL mode creates a kernel thread that continuously polls the submission queue:\nnew ReactorConfig( RingFlags: ABI.IORING_SETUP_SQPOLL | ABI.IORING_SETUP_SQ_AFF | ABI.IORING_SETUP_SINGLE_ISSUER, SqCpuThread: 4, // pin to CPU 4 SqThreadIdleMs: 200 // sleep after 200ms idle ) Benefits:\nEliminates the io_uring_enter() syscall for submissions Reduced per-submission latency Costs:\nDedicates one CPU core per reactor Increased power consumption Requires CAP_SYS_NICE or appropriate permissions in containers When to enable:\nYou have spare cores (total cores \u003e 2 * reactor count) Submission latency is your bottleneck You’re already saturating network bandwidth When to avoid:\nCore-constrained environments (containers, small VMs) Acceptor ring (multishot accept generates CQEs from interrupts, not submissions)","system-tuning#System Tuning":"# Increase file descriptor limit ulimit -n 1000000 # Increase somaxconn (listen backlog limit) sysctl -w net.core.somaxconn=65535 # Increase local port range (for clients) sysctl -w net.ipv4.ip_local_port_range=\"1024 65535\" # Disable TCP timestamps (minor latency improvement) sysctl -w net.ipv4.tcp_timestamps=0"},"title":"Performance Tuning"},"/zerg/docs/guides/zero-allocation/":{"data":{"":"zerg is designed for allocation-free operation on the hot path. This guide explains the read/write patterns and their allocation characteristics.","allocation-free-patterns-summary#Allocation-Free Patterns Summary":"Pattern Allocation? When to Use ring.AsSpan() None Reading a single buffer in-place TryGetRing() loop None Iterating buffers one at a time GetAllSnapshotRingsAsUnmanagedMemory() Array When you need ReadOnlySequence for multi-segment parsing TryDynamicallyGetAllSnapshotRings() List (if data) When ring count varies and you want out semantics connection.Write(span) None Staging response bytes connection.GetSpan() + Advance() None Direct-write via IBufferWriter","choosing-a-read-pattern#Choosing a Read Pattern":"Scenario Pattern Allocation Single buffer, simple protocol TryGetRing() + AsSpan() None Multi-buffer or complex parsing GetAllSnapshotRingsAsUnmanagedMemory() + ToReadOnlySequence() Array + segments Need individual ring control TryGetRing() loop None BCL compatibility ConnectionStream.ReadAsync() Copies to managed buffer","direct-span-write-zero-allocation#Direct Span Write (Zero Allocation)":"connection.Write(\"HTTP/1.1 200 OK\\r\\nContent-Length: 13\\r\\n\\r\\nHello, World!\"u8); await connection.FlushAsync(); Using u8 string literals produces compile-time constant UTF-8 data in the assembly’s read-only section. No allocation.","ibufferwriter-zero-allocation#IBufferWriter (Zero Allocation)":"Write directly into the connection’s slab without intermediate buffers:\nSpan\u003cbyte\u003e span = connection.GetSpan(256); int written = FormatResponse(span); connection.Advance(written); await connection.FlushAsync();","reading-with-readonlysequence#Reading with ReadOnlySequence":"When data spans multiple kernel buffers, use GetAllSnapshotRingsAsUnmanagedMemory to build a ReadOnlySequence for SequenceReader based parsing:\nstatic async Task HandleConnectionAsync(Connection connection) { while (true) { var result = await connection.ReadAsync(); if (result.IsClosed) break; // Get all received buffers as managed memory var rings = connection.GetAllSnapshotRingsAsUnmanagedMemory(result); // Build a ReadOnlySequence for multi-segment parsing ReadOnlySequence\u003cbyte\u003e sequence = rings.ToReadOnlySequence(); var reader = new SequenceReader\u003cbyte\u003e(sequence); // Parse the request... ProcessSequence(connection, ref reader); // Return all buffers rings.ReturnRingBuffers(connection.Reactor); await connection.FlushAsync(); connection.ResetRead(); } } This allocates one UnmanagedMemoryManager[] array and one RingSegment per buffer, but the data itself is never copied.","reading-with-readonlyspan#Reading with ReadOnlySpan":"The simplest approach: iterate ring items one at a time using TryGetRing and process each as a ReadOnlySpan. Zero allocation.\nstatic async Task HandleConnectionAsync(Connection connection) { while (true) { var result = await connection.ReadAsync(); if (result.IsClosed) break; while (connection.TryGetRing(result.TailSnapshot, out RingItem ring)) { ReadOnlySpan\u003cbyte\u003e data = ring.AsSpan(); // Parse and respond in-place -- no allocation ProcessRequest(connection, data); // Return buffer immediately connection.ReturnRing(ring.BufferId); } await connection.FlushAsync(); connection.ResetRead(); } } Key patterns:\nUse TryGetRing to iterate one buffer at a time Use ring.AsSpan() for zero-copy access to the data Return each buffer immediately after processing No arrays, lists, or sequences allocated","real-world-example#Real-World Example":"The TechEmpower benchmark handler in the zerg repository demonstrates these patterns in a production HTTP server.","writing-responses#Writing Responses":""},"title":"Zero-Allocation Patterns"},"/zerg/docs/internals/":{"data":{"":"Deep dive into zerg’s internal data structures, native interop layer, and memory management.\nSPSC Recv RingLock-free single-producer single-consumer ring buffer MPSC QueuesMpscUshortQueue, MpscIntQueue, and MpscRecvRing Native Interopliburingshim, P/Invoke bindings, and ABI layer Memory ManagementUnmanaged slabs, pooling, and UnmanagedMemoryManager"},"title":"Internals"},"/zerg/docs/internals/memory-management/":{"data":{"":"zerg minimizes GC pressure by using unmanaged memory for all hot-path data: receive buffers, write slabs, and inflight buffers. This page describes the memory management strategy.","buffer-ring-slab#Buffer Ring Slab":"Each reactor pre-allocates a contiguous slab for receive buffers:\n┌────────────────────────────────────────────────────────────┐ │ Buffer Ring Slab │ │ ┌──────────┬──────────┬──────────┬─────┬──────────┐ │ │ │ Buffer 0 │ Buffer 1 │ Buffer 2 │ ... │ Buffer N │ │ │ │ 32 KB │ 32 KB │ 32 KB │ │ 32 KB │ │ │ └──────────┴──────────┴──────────┴─────┴──────────┘ │ │ ^ slab │ │ ^ slab + 0 * bufSize │ │ ^ slab + 1 * bufSize │ │ ^ slab + 2 * bufSize │ └────────────────────────────────────────────────────────────┘ Address formula: bufferPtr = slab + bufferId * RecvBufferSize\nThe kernel writes directly into these buffers via the buffer ring. When the handler is done, the buffer ID is returned and the same slot is reused for future receives.","connection-pooling#Connection Pooling":"Connections can be pooled to avoid repeated unmanaged allocation. The reset methods prepare a connection for reuse:\nMethod Speed Safety Use When Clear() Slower Cancels pending waiters Connection may have outstanding async operations Clear2() Faster No waiter cancellation Handler has definitely exited Both methods:\nIncrement _generation (invalidates stale ValueTask tokens) Set _closed = 1 Clear the SPSC receive ring Reset write buffer state The generation counter is the key safety mechanism: any ValueTask created before the reset will observe a mismatched token and return Closed instead of accessing stale data.","constructors#Constructors":"Constructor Freeable Use Case (byte* ptr, int length) Yes Owned unmanaged allocations (byte* ptr, int length, bool freeable) Configurable Borrowed pointers (e.g., buffer ring) (byte* ptr, int length, ushort bufferId) Yes Recv buffers with buffer ring ID (byte* ptr, int length, ushort bufferId, bool freeable) Configurable Full control For buffer ring receive data, freeable is typically false because the buffer belongs to the reactor’s slab and is returned, not freed.","disposal#Disposal":"public void Dispose() { _manager.Free(); // NativeMemory.AlignedFree _manager.Dispose(); }","gc-pressure-analysis#GC Pressure Analysis":"Operation Allocations Notes ReadAsync() 0 ValueTask-based, no state machine allocation FlushAsync() 0 ValueTask-based Write(span) 0 Direct memcpy to unmanaged slab GetSpan() + Advance() 0 Direct pointer arithmetic TryGetRing() 0 Returns struct by value ReturnRing() 0 Enqueues ushort to MPSC queue ring.AsSpan() 0 Creates Span over existing pointer GetAllSnapshotRingsAsUnmanagedMemory() 1 array Allocates UnmanagedMemoryManager[] ToReadOnlySequence() N segments Allocates RingSegment per buffer ConnectionStream.ReadAsync() 1 array + segments Copies data to managed buffer For the lowest allocation rate, use TryGetRing() in a loop and process data via ring.AsSpan().","memory-copy-helpers#Memory Copy Helpers":"The MemoryExtensions class provides optimized copy operations:\n// Copy from native pointer to managed Memory void CopyFrom(this Memory\u003cbyte\u003e dst, byte* src, int len) // Copy from RingItem to managed Memory void CopyFromRing(this Memory\u003cbyte\u003e dst, ref RingItem ring) // Copy from array of RingItems to managed Memory int CopyFromRings(this Memory\u003cbyte\u003e dst, RingItem[] ring) All use Buffer.MemoryCopy for efficient native-to-managed copying with bounds checking.","readonlysequence-construction#ReadOnlySequence Construction":"When received data spans multiple kernel buffers, UnmanagedMemoryManager instances are linked into a ReadOnlySequence:\npublic static ReadOnlySequence\u003cbyte\u003e ToReadOnlySequence(this UnmanagedMemoryManager[] managers) This creates a chain of RingSegment objects (custom ReadOnlySequenceSegment subclass) that link the managed views:\n[Manager0.Memory] → [Manager1.Memory] → [Manager2.Memory] ↓ ↓ ↓ Segment0 ──next──▶ Segment1 ──next──▶ Segment2 The resulting ReadOnlySequence can be parsed with SequenceReader for efficient multi-segment processing.","unmanaged-allocations#Unmanaged Allocations":"All performance-critical buffers are allocated outside the managed heap:\nComponent Size Alignment Lifetime Buffer ring slab BufferRingEntries * RecvBufferSize per reactor 64 bytes Reactor lifetime Write slab 16 KB per connection (configurable) 64 bytes Connection lifetime Inflight buffer User-defined (typically 16 KB) per handler 64 bytes Handler lifetime","unmanagedmemorymanager#UnmanagedMemoryManager":"The UnmanagedMemoryManager class bridges unmanaged pointers and .NET’s managed memory model:\npublic sealed unsafe class UnmanagedMemoryManager : MemoryManager\u003cbyte\u003e { private byte* _ptr; private int _length; private ushort BufferId; private bool _freeable; public Span\u003cbyte\u003e GetSpan() =\u003e new Span\u003cbyte\u003e(_ptr, _length); public MemoryHandle Pin(int elementIndex = 0) =\u003e new MemoryHandle(_ptr + elementIndex); public void Unpin() { } // no-op: already unmanaged public void Free() { if (_freeable) NativeMemory.AlignedFree(_ptr); } } This enables zero-copy interop between kernel buffers and .NET APIs that accept Memory, ReadOnlyMemory, or ReadOnlySequence.","usage-pattern#Usage Pattern":"// Wrap kernel buffer in managed view var manager = new UnmanagedMemoryManager(ring.Ptr, ring.Length, ring.BufferId, freeable: false); ReadOnlyMemory\u003cbyte\u003e memory = manager.Memory; // zero allocation","why-64-byte-alignment#Why 64-Byte Alignment?":"64 bytes is a common L1 cache line size on x86_64. Aligned allocations prevent false sharing and ensure optimal memory access patterns for DMA operations used by the kernel.\nbyte* ptr = (byte*)NativeMemory.AlignedAlloc((nuint)size, 64); // ... NativeMemory.AlignedFree(ptr);","write-slab#Write Slab":"Each connection owns a write slab (default 16 KB):\npublic Connection(int writeSlabSize = 1024 * 16) { _writeSlabSize = writeSlabSize; _manager = new UnmanagedMemoryManager( (byte*)NativeMemory.AlignedAlloc((nuint)writeSlabSize, 64), writeSlabSize ); } The slab lifecycle:\nWrite(data) → Advance WriteTail → FlushAsync() → Reactor sends → Reset(Head=0, Tail=0) The slab is never freed and reallocated during the connection’s lifetime – it’s allocated once and reused."},"title":"Memory Management"},"/zerg/docs/internals/mpsc-queues/":{"data":{"":"zerg uses three MPSC (multi-producer, single-consumer) queue implementations for cross-thread communication where multiple handler threads need to send messages to a single reactor thread.","algorithm-interlocked-tail#Algorithm: Interlocked Tail":"Producer: 1. Volatile read head and tail for fast full-check 2. slot = Interlocked.Increment(ref _tail) - 1 // full fence, reserve slot 3. _items[slot \u0026 _mask] = item // store 4. return true Consumer: 1. snapshot = Volatile.Read(ref _tail) 2. while _head \u003c snapshot: item = _items[_head \u0026 _mask] _head++ This is simpler than the sequence-per-slot algorithm: producers just increment the tail atomically, and the consumer trusts the tail as an upper bound.\nTrade-off: There’s a small window where _tail has been incremented but the item isn’t stored yet. The consumer relies on the SnapshotTail() / TryDequeueUntil() pattern to avoid reading partially-written slots.","algorithm-sequence-per-slot#Algorithm: Sequence-Per-Slot":"This implements a variant of Dmitry Vyukov’s bounded MPSC queue. Each slot has a sequence number that coordinates ownership between producers and the consumer.\nProducer (TryEnqueue):\n1. ticket = Interlocked.Increment(ref _tail) - 1 // reserve unique slot 2. idx = ticket \u0026 _mask // compute array index 3. if _seq[idx] != ticket: return false // slot not ready (full) 4. _data[idx] = value // store value 5. Volatile.Write(ref _seq[idx], ticket + 1) // publish (release) The key insight: each producer gets a unique ticket via Interlocked.Increment. If the slot’s sequence matches the ticket, the slot is free. After storing the value, the producer advances the sequence by 1 to signal “ready for consumption.”\nConsumer (TryDequeue):\n1. head = _head // plain read (single consumer) 2. idx = head \u0026 _mask 3. if _seq[idx] != head + 1: return false // not ready yet 4. value = _data[idx] // extract value 5. Volatile.Write(ref _seq[idx], head + _capacity) // mark free for next wrap 6. _head = head + 1 // advance (plain write) The consumer checks if the sequence is head + 1 (one ahead of its position, meaning a producer has filled it). After reading, it resets the sequence to head + capacity so the slot is ready for the next cycle.\nBlocking variant (EnqueueSpin):\npublic void EnqueueSpin(ushort value) Spins with SpinWait backoff until TryEnqueue succeeds. Used when buffer returns must not be dropped.","api#API":"public bool TryEnqueue(int item) // CAS-based multi-producer enqueue public bool TryDequeue(out int item) // single-consumer dequeue public int CountApprox { get; } // approximate queue depth The TryEnqueue implementation uses Interlocked.CompareExchange in a loop to claim a slot, making it lock-free but not wait-free under contention.","api-1#API":"public bool TryEnqueue(in RingItem item) public long SnapshotTail() public bool TryDequeueUntil(long tailSnapshot, out RingItem item) public RingItem DequeueSingle() public bool IsEmpty() public void Clear()","cache-line-considerations#Cache Line Considerations":"The MpscIntQueue pads both _enqueuePos and _dequeuePos to 64 bytes to prevent false sharing. When a producer writes to _enqueuePos and the consumer reads _dequeuePos, they should be on different cache lines to avoid invalidation traffic.\nThe ring-based MPSC queues (MpscRecvRing, MpscWriteItem) don’t pad their head/tail fields separately, relying on the item array itself to provide spatial separation.","choosing-between-algorithms#Choosing Between Algorithms":"Algorithm Pros Cons Used When Sequence-per-slot (Vyukov) No torn reads, exact capacity CAS contention under high producer count Small value types (ushort, int) Interlocked tail Simpler, faster under low contention Small visibility window Struct items (RingItem, WriteItem)","design#Design":"public sealed class MpscUshortQueue { private int _capacity; // power of 2 private int _mask; // capacity - 1 private long[] _seq; // per-slot sequence numbers private ushort[] _data; // actual values private long _tail; // producer position (contended) private long _head; // consumer position (single reader) }","design-1#Design":"public sealed unsafe class MpscRecvRing { private RingItem[] _items; private int _mask; private long _tail; // producers: Interlocked.Increment private long _head; // consumer: plain read/write }","drain#Drain":"public int Drain(Action\u003cushort\u003e consume, int max = int.MaxValue) Dequeues up to max items, calling the action for each. Returns count drained.","mpscintqueue#MpscIntQueue":"Same algorithm as MpscUshortQueue, but stores int values. Used for flush requests (handler enqueues clientFd to the reactor’s flush queue).","mpscrecvring#MpscRecvRing":"A simpler MPSC ring buffer using Interlocked.Increment for slot reservation. Used when multiple producers need to enqueue RingItem structs.","mpscushortqueue#MpscUshortQueue":"Used by handlers to return buffer IDs to the reactor after processing received data.","mpscwriteitem#MpscWriteItem":"Same structure as MpscRecvRing but stores WriteItem structs (buffer + client fd pairs).","overview#Overview":"Queue Item Type Used For Algorithm MpscUshortQueue ushort Buffer ID returns Sequence-per-slot (Vyukov variant) MpscIntQueue int Flush requests (client fds) Sequence-per-slot (Vyukov) MpscRecvRing RingItem Multi-producer recv enqueue Interlocked tail increment MpscWriteItem WriteItem Write item queue Interlocked tail increment","structure#Structure":"public sealed class MpscIntQueue { [StructLayout(LayoutKind.Explicit, Size = 64)] struct PaddedLong { [FieldOffset(0)] public long Value; } struct Cell { long Sequence; int Value; } private Cell[] _buffer; private PaddedLong _enqueuePos; // 64-byte padded to prevent false sharing private PaddedLong _dequeuePos; // 64-byte padded to prevent false sharing } The PaddedLong struct ensures the enqueue and dequeue positions are on separate cache lines (64 bytes), preventing false sharing between producer and consumer threads."},"title":"MPSC Queues"},"/zerg/docs/internals/native-interop/":{"data":{"":"zerg communicates with the Linux kernel’s io_uring subsystem through a C shim library (liburingshim.so) that wraps liburing. All native calls are made via P/Invoke from the ABI.URing class.","__kernel_timespec#__kernel_timespec":"[StructLayout(LayoutKind.Sequential)] internal struct __kernel_timespec { public long tv_sec; // seconds public long tv_nsec; // nanoseconds } Matches the Linux kernel’s struct __kernel_timespec (16 bytes). Used for timeout operations.","architecture#Architecture":"C# (zerg) ──P/Invoke──▶ liburingshim.so ──calls──▶ liburing ──syscall──▶ Linux kernel The shim library serves two purposes:\nProvides a flat C ABI that P/Invoke can call (liburing uses inline functions and macros) Bundles liburing statically so users don’t need to install it","buffer-ring#Buffer Ring":"Function Description shim_setup_buf_ring(ring, entries, bgid, flags, out ret) Create and register buffer ring shim_free_buf_ring(ring, br, entries, bgid) Unregister and free shim_buf_ring_add(br, addr, len, bid, mask, idx) Add buffer to ring shim_buf_ring_advance(br, count) Publish N buffers to kernel shim_cqe_has_buffer(cqe) Check if CQE used a provided buffer shim_cqe_buffer_id(cqe) Extract buffer ID from CQE flags","bundled-native-libraries#Bundled Native Libraries":"The NuGet package includes pre-built shim libraries:\nRuntime Path linux-x64 (glibc) native/linux-x64/liburingshim.so linux-musl-x64 (Alpine) native/linux-musl-x64/liburingshim.so These are automatically copied to the output directory by MSBuild.","completion#Completion":"Function Description shim_wait_cqe(ring, cqe) Blocking wait for one CQE shim_wait_cqe_timeout(ring, cqe, ts) Wait with timeout shim_wait_cqes(ring, cqe, waitNr, ts) Wait for N CQEs shim_peek_batch_cqe(ring, cqes, count) Non-blocking batch peek shim_cqe_seen(ring, cqe) Mark one CQE consumed shim_cq_advance(ring, count) Mark N CQEs consumed (batch) shim_cq_ready(ring) Check CQE count (no syscall) shim_sq_ready(ring) Check available SQE slots","cpu-affinity#CPU Affinity":"The ABI.Affinity class provides thread-to-core pinning:\nAffinity.PinCurrentThreadToCpu(int cpu) Uses sched_setaffinity(2) with a CPU bitmask. Best-effort – failures are logged but not fatal.","io_uring#io_uring":"[StructLayout(LayoutKind.Sequential)] internal unsafe struct io_uring { private fixed ulong _opaque[128]; // 1024 bytes } Managed equivalent of struct io_uring from liburing. Never directly manipulated – passed to/from shim functions by pointer.","io_uring_buf_ring#io_uring_buf_ring":"[StructLayout(LayoutKind.Sequential)] internal struct io_uring_buf_ring { } Opaque token for the buffer ring. All manipulation via shim functions.","io_uring_cqe#io_uring_cqe":"[StructLayout(LayoutKind.Sequential)] internal unsafe struct io_uring_cqe { public ulong user_data; // 64-bit token from SQE public int res; // result (bytes transferred or -errno) public uint flags; // CQE flags (buffer selection, multishot) } Completion Queue Entry. Read directly from C# after peek/wait.","io_uring_sqe#io_uring_sqe":"[StructLayout(LayoutKind.Sequential)] internal unsafe struct io_uring_sqe { private fixed ulong _opaque[8]; // 64 bytes } Submission Queue Entry. Populated via shim_prep_* helpers. Never directly filled from C#.","opaque-types#Opaque Types":"The C# side uses fixed-size structs as opaque handles for kernel types:","pinvoke-surface#P/Invoke Surface":"","ring-lifecycle#Ring Lifecycle":"Function Description shim_create_ring(entries, out err) Create ring with SQ/CQ size shim_create_ring_ex(entries, flags, cpu, idle_ms, out err) Create ring with flags (SQPOLL, etc.) shim_destroy_ring(ring) Release all native resources shim_get_ring_flags(ring) Get ring setup flags","socket-operations#Socket Operations":"The ABI.LinuxSocket class provides raw socket syscall wrappers:\nFunction Description socket(domain, type, proto) Create socket bind(fd, addr, len) Bind to address (IPv4 and IPv6 overloads) listen(fd, backlog) Mark as listening setsockopt(fd, level, optname, optval, optlen) Set socket option close(fd) Close file descriptor fcntl(fd, cmd, arg) File control (flags) inet_pton(af, src, dst) Text to binary IP address","sqe-preparation#SQE Preparation":"Function Description shim_prep_multishot_accept(sqe, lfd, flags) Multishot accept on listening fd shim_prep_recv_multishot_select(sqe, fd, buf_group, flags) Multishot recv with buffer selection shim_prep_send(sqe, fd, buf, nbytes, flags) Send data from buffer shim_prep_cancel64(sqe, user_data, flags) Cancel operation by user_data","submission#Submission":"Function Description shim_get_sqe(ring) Acquire fresh SQE slot (null if full) shim_submit(ring) Submit pending SQEs to kernel shim_submit_and_wait(ring, waitNr) Submit + wait for CQEs (single syscall) shim_submit_and_wait_timeout(ring, cqes, waitNr, ts) Submit + wait with timeout shim_enter(ring, toSubmit, minComplete, flags, ts) Direct io_uring_enter(2)","user-data#User Data":"Function Description shim_sqe_set_data64(sqe, data) Set 64-bit token on SQE shim_cqe_get_data64(cqe) Get token from CQE","user-data-token-packing#User Data Token Packing":"zerg encodes the operation kind and file descriptor into the 64-bit user_data field:\ninternal enum UdKind : uint { Accept = 1, Recv = 2, Send = 3, Cancel = 4 } static ulong PackUd(UdKind k, int fd) =\u003e ((ulong)k \u003c\u003c 32) | (uint)fd; static UdKind UdKindOf(ulong ud) =\u003e (UdKind)(ud \u003e\u003e 32); static int UdFdOf(ulong ud) =\u003e (int)(ud \u0026 0xFFFFFFFF); This allows the reactor to dispatch CQEs to the correct handler (recv, send, cancel) in a single branch on the extracted kind."},"title":"Native Interop"},"/zerg/docs/internals/spsc-ring/":{"data":{"":"The SpscRecvRing is the per-connection inbound data queue. It’s a lock-free, single-producer single-consumer ring buffer optimized for the reactor-to-handler data path.","api#API":"","consumer-handler-thread#Consumer (Handler Thread)":"public long SnapshotTail() Volatile-reads _tail to capture the current batch boundary. The handler can drain items up to this position without observing partially-written state.\npublic bool TryDequeueUntil(long tailSnapshot, out RingItem item) Dequeues one item, bounded by the snapshot. Returns false when _head \u003e= tailSnapshot.\nCompares _head against tailSnapshot Reads _items[_head \u0026 _mask] Advances _head (plain write – only consumer writes _head) public RingItem DequeueSingle() Unconditional dequeue. Assumes the ring is not empty. Direct read and advance.","consumer-side-acquire#Consumer Side (Acquire)":"long tail = Volatile.Read(ref _tail); // acquire fence var item = _items[_head \u0026 _mask]; // load item The volatile read of _tail ensures the consumer sees all stores made by the producer up to that tail position.","design#Design":"public sealed class SpscRecvRing { private RingItem[] _items; // power-of-2 capacity array private int _mask; // capacity - 1 private long _tail; // producer write position private long _head; // consumer read position } Capacity: 1024 items (hardcoded in connection construction) Producer: Reactor thread (enqueues RingItem on recv CQE) Consumer: Handler task (drains items in snapshot batches)","inspection#Inspection":"public bool IsEmpty() // volatile reads head and tail public long GetTailHeadDiff() // approximate count (tail - head) public void Clear() // volatile reset both to 0","memory-ordering#Memory Ordering":"The SPSC ring relies on two key ordering guarantees:","performance-characteristics#Performance Characteristics":"Operation Cost Allocation TryEnqueue ~5 ns None TryDequeueUntil ~3 ns None SnapshotTail ~1 ns None IsEmpty ~2 ns None All operations are [MethodImpl(MethodImplOptions.AggressiveInlining)] for JIT inlining.","producer-reactor-thread#Producer (Reactor Thread)":"public bool TryEnqueue(in RingItem item) Enqueues one item. Returns false if the ring is full.\nVolatile-reads _head to check capacity (_tail - _head \u003e= capacity → full) Stores item at _items[_tail \u0026 _mask] Volatile-writes _tail = _tail + 1 (release semantics: ensures item is visible before tail advances)","producer-side-release#Producer Side (Release)":"_items[_tail \u0026 _mask] = item; // store item Volatile.Write(ref _tail, _tail + 1); // publish tail (release fence) The volatile write to _tail ensures that the item store is visible to the consumer before the tail advance is visible. The consumer will never see an advanced tail with a stale item.","ring-full-behavior#Ring Full Behavior":"When the SPSC ring is full (1024 items waiting to be consumed by the handler):\nThe reactor’s EnqueueRingItem() detects the ring is full The connection is marked as closed (_closed = 1) If the handler is armed, it’s woken with a close signal If no handler is armed, _pending is set This is a safety measure – a handler that falls behind and doesn’t drain its ring will eventually have its connection closed rather than consuming unbounded kernel buffers.","single-writer-optimization#Single-Writer Optimization":"Since only the consumer writes _head and only the producer writes _tail, these fields don’t need atomic operations. Plain reads from the owning thread and volatile reads from the other thread are sufficient.","snapshot-based-batching#Snapshot-Based Batching":"The snapshot pattern prevents the consumer from chasing a moving tail:\n// Handler side: ReadResult result = await connection.ReadAsync(); long snapshot = result.TailSnapshot; // captured once // Drain exactly what was available at ReadAsync time while (connection.TryGetRing(snapshot, out RingItem ring)) { // process ring... connection.ReturnRing(ring.BufferId); } // Guaranteed to terminate: snapshot is fixed, head advances toward it This is important because the reactor may enqueue more items while the handler is processing. Without a snapshot boundary, the handler could spin indefinitely.","why-spsc#Why SPSC?":"Each connection is owned by exactly one reactor. The reactor is the sole producer, and the handler awaiting ReadAsync() is the sole consumer. This single-producer/single-consumer invariant means:\nNo atomics needed on _tail or _head for their respective owners Volatile reads/writes are sufficient for cross-thread visibility No CAS loops, no contention, no backoff Maximum throughput with minimal overhead"},"title":"SPSC Recv Ring"}}